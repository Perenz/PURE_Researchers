[{"bib": {"title": "MonetDB/XQuery: a fast XQuery processor powered by a relational engine", "pub_year": 2006, "author": "Peter Boncz and Torsten Grust and Maurice van Keulen and Stefan Manegold and Jan Rittinger and Jens Teubner", "conference": "Proceedings of the 2006 ACM SIGMOD international conference on Management of data", "pages": "479-490", "publisher": "ACM", "abstract": "Relational XQuery systems try to re-use mature relational data management infrastructures to create fast and scalable XML database technology. This paper describes the main features, key contributions, and lessons learned while implementing such a system. Its architecture consists of (i) a range-based encoding of XML documents into relational tables,(ii) a compilation technique that translates XQuery into a basic relational algebra,(iii) a restricted (order) property-aware peephole relational query optimization strategy, and (iv) a mapping from XML update statements into relational updates. Thus, this system implements all essential XML database functionalities (rather than a single feature) such that we can learn from the full consequences of our architectural decisions. While implementing this system, we had to extend the state-of-the-art with a number of new technical contributions, such as loop-lifted staircase \u2026"}}, {"bib": {"title": "Staircase join: Teach a relational DBMS to watch its (axis) steps", "pub_year": 2003, "author": "Torsten Grust and Maurice van Keulen and Jens Teubner", "conference": "Proceedings of the 29th international conference on Very large data bases-Volume 29", "pages": "524-535", "publisher": "VLDB Endowment", "abstract": "Relational query processors derive much of their effectiveness from the awareness of specific table properties like sort order, size, or absence of duplicate tuples. This chapter applies (and adapts) this successful principle to database-supported XML and XPath processing: the relational system is made tree aware, i.e., tree properties like subtree size, intersection of paths, inclusion, or disjointness of subtrees are made explicit. The staircase join operator described in this chapter exploits the tree properties encoded in the pre/post plane to optimize database-supported XPath evaluation. Knowledge of tree properties like subtree size and inclusion/disjointness of subtrees is available in this encoding at the cost of simple integer operations. This chapter explores that the increased tree awareness can lead to significantly improved XPath performance. To avoid cluttering the query optimizer with XML \u2026"}}, {"bib": {"title": "Accelerating XPath evaluation in any RDBMS", "pub_year": 2004, "author": "Torsten Grust and Maurice van Keulen and Jens Teubner", "journal": "ACM Transactions on Database Systems (TODS)", "volume": "29", "number": "1", "pages": "91-131", "publisher": "ACM", "abstract": "This article is a proposal for a database index structure, the XPath accelerator, that has been specifically designed to support the evaluation of XPath path expressions. As such, the index is capable to support all XPath axes (including ancestor, following, preceding-sibling, descendant-or-self, etc.). This feature lets the index stand out among related work on XML indexing structures which had a focus on the child and descendant axes only. The index has been designed with a close eye on the XPath semantics as well as the desire to engineer its internals so that it can be supported well by existing relational database query processing technology: the index (a) permits set-oriented (or, rather, sequence-oriented) path evaluation, and (b) can be implemented and queried using well-established relational index structures, notably B-trees and R-trees.We discuss the implementation of the XPath accelerator on top of \u2026"}}, {"bib": {"title": "A probabilistic XML approach to data integration", "pub_year": 2005, "author": "Maurice Van Keulen and Ander De Keijzer and Wouter Alink", "conference": "21st International Conference on Data Engineering (ICDE'05)", "pages": "459-470", "publisher": "IEEE", "abstract": "In mobile and ambient environments, devices need to become autonomous, managing and resolving problems without interference from a user. The database of a (mobile) device can be seen as its knowledge about objects in the 'real world'. Data exchange between small and/or large computing devices can be used to supplement and update this knowledge whenever a connection gets established. In many situations, however, data from different data sources referring to the same real world objects, may conflict. It is the task of the data management system of the device to resolve such conflicts without interference from a user. In this paper, we take a first step in the development of a probabilistic XML DBMS. The main idea is to drop the assumption that data in the database should be certain: subtrees in XML documents may denote possible views on the real world. We formally define the notion of probabilistic XML \u2026"}}, {"bib": {"title": "Qualitative effects of knowledge rules and user feedback in probabilistic data integration", "pub_year": 2009, "author": "Maurice van Keulen and Ander de Keijzer", "journal": "The VLDB Journal", "volume": "18", "number": "5", "pages": "1191-1217", "publisher": "Springer-Verlag", "abstract": "In data integration efforts, portal development in particular, much development time is devoted to entity resolution. Often advanced similarity measurement techniques are used to remove semantic duplicates or solve other semantic conflicts. It proves impossible, however, to automatically get rid of all semantic problems. An often-used rule of thumb states that about 90% of the development effort is devoted to semi-automatically resolving the remaining 10% hard cases. In an attempt to significantly decrease human effort at data integration time, we have proposed an approach that strives for a \u2018good enough\u2019 initial integration which stores any remaining semantic uncertainty and conflicts in a probabilistic database. The remaining cases are to be resolved with user feedback during query time. The main contribution of this paper is an experimental investigation of the effects and sensitivity of rule definition \u2026"}}, {"bib": {"title": "Pathfinder: XQuery---the relational way", "pub_year": 2005, "author": "Peter Boncz and Torsten Grust and Maurice van Keulen and Stefan Manegold and Jan Rittinger and Jens Teubner", "conference": "Proceedings of the 31st international conference on Very large data bases", "pages": "1322-1325", "publisher": "VLDB Endowment", "abstract": "Relational query processors are probably the best understood (as well as the best engineered) query engines available today. Although carefully tuned to process instances of the relational model (tables of tuples), these processors can also provide a foundation for the evaluation of \u201calien\u201d(non-relational) query languages: if a relational encoding of the alien data model and its associated query language is given, the RDBMS may act like a special-purpose processor for the new language.This demonstration features our XQuery compiler Pathfinder, the continuation of our earlier work on a purely relational XPath and XQuery processing stack [4, 6, 7] in which we developed relational encodings and processing strategies for the tree-shaped XML data model. The Pathfinder project is an exploration of how far we can push the idea of using mature RDBMS technology to design and build a full-fledged XQuery implementation. The demonstration will show that this line of research was and still is worth to be followed: based on the extensible relational database kernel MonetDB [2], Pathfinder provides highly efficient and scalable XQuery technology that scales beyond 10 GB XML input instances on commodity hardware. Pathfinder requires only local extensions to the underlying DBMS\u2019s kernel, such as the staircase join operator[7, 9]. A join recognition logic in our compiler, as well as a careful consideration of order properties of relational operators [3], allow for effective optimizations that turn MonetDB into a highly efficient XQuery engine."}}, {"bib": {"title": "Process prediction in noisy data sets: a case study in a dutch hospital", "pub_year": 2012, "author": "Sjoerd Van Der Spoel and Maurice Van Keulen and Chintan Amrit", "conference": "International Symposium on Data-Driven Process Discovery and Analysis", "pages": "60-83", "publisher": "Springer Berlin Heidelberg", "abstract": "Predicting the amount of money that can be claimed is critical to the effective running of an Hospital. In this paper we describe a case study of a Dutch Hospital where we use process mining to predict the cash flow of the Hospital. In order to predict the cost of a treatment, we use different data mining techniques to predict the sequence of treatments administered, the duration and the final \u201dcare product\u201d or diagnosis of the patient. While performing the data analysis we encountered three specific kinds of noise that we call sequence noise, human noise and duration noise. Studies in the past have discussed ways to reduce the noise in process data. However, it is not very clear what effect the noise has to different kinds of process analysis. In this paper we describe the combined effect of sequence noise, human noise and duration noise on the analysis of process data, by comparing the performance of several \u2026"}}, {"bib": {"title": "Tree awareness for relational DBMS kernels: Staircase join", "pub_year": 2003, "author": "Torsten Grust and Maurice Van Keulen", "pages": "231-245", "publisher": "Springer Berlin Heidelberg", "abstract": "Relational database management systems (RDBMSs) derive much of their efficiency from the versatility of their core data structure: tables of tuples. Such tables are simple enough to allow for an efficient representation on all levels of the memory hierarchy, yet sufficiently generic to host a wide range of data types. If one can devise mappings from a data type \u03c4 to tables and from operations on \u03c4 to relational queries, an RDBMS may be a premier implementation alternative. Temporal intervals, complex nested objects, and spatial data are sample instances for such types \u03c4."}}, {"bib": {"title": "ROX: run-time optimization of XQueries.", "pub_year": 2009, "author": "Riham Abdel Kader and Peter A Boncz and Stefan Manegold and Maurice Van Keulen", "conference": "SIGMOD Conference", "pages": "615-626", "abstract": "Optimization of complex XQueries combining many XPath steps and joins is currently hindered by the absence of good cardinality estimation and cost models for XQuery. Additionally, the state-ofthe-art of even relational query optimization still struggles to cope with cost model estimation errors that increase with plan size, as well as with the effect of correlated joins and selections. In this research, we propose to radically depart from the traditional path of separating the query compilation and query execution phases, by having the optimizer execute, materialize partial results, and use sampling based estimation techniques to observe the characteristics of intermediates. The proposed technique takes as input a Join Graph where the edges are either equi-joins or XPath steps, and the execution environment provides value-and structural-join algorithms, as well as structural and value-based indices. While run-time optimization with sampling removes many of the vulnerabilities of classical optimizers, it brings its own challenges with respect to keeping resource usage under control, both with respect to the materialization of intermediates, as well as the cost of plan exploration using sampling. Our approach deals with these issues by limiting the run-time search space to so-called \u201czeroinvestment\u201d algorithms for which sampling can be guaranteed to be strictly linear in sample size. All operators and XML value indices used by ROX for sampling have the zero-investment property. We perform extensive experimental evaluation on large XML datasets that shows that our run-time query optimizer finds good query plans in a robust fashion and has limited run \u2026"}}, {"bib": {"title": "Big Data Semantics", "pub_year": 2018, "author": "Paolo Ceravolo and Antonia Azzini and Marco Angelini and Tiziana Catarci and Philippe Cudr\u00e9-Mauroux and Ernesto Damiani and Alexandra Mazak and Maurice Van Keulen and Mustafa Jarrar and Giuseppe Santucci and Kai-Uwe Sattler and Monica Scannapieco and Manuel Wimmer and Robert Wrembel and Fadi Zaraket", "journal": "Journal on Data Semantics", "volume": "7", "number": "2", "pages": "65-85", "publisher": "Springer Berlin Heidelberg", "abstract": "Big Data technology has discarded traditional data modeling approaches as no longer applicable to distributed data processing. It is, however, largely recognized that Big Data impose novel challenges in data and infrastructure management. Indeed, multiple components and procedures must be coordinated to ensure a high level of data quality and accessibility for the application layers, e.g., data analytics and reporting. In this paper, the third of its kind co-authored by members of IFIP WG 2.6 on Data Semantics, we propose a review of the literature addressing these topics and discuss relevant challenges for future research. Based on our literature review, we argue that methods, principles, and perspectives developed by the Data Semantics community can significantly contribute to address Big Data challenges."}}, {"bib": {"title": "Using element clustering to increase the efficiency of xml schema matching", "pub_year": 2006, "author": "Marko Smiljanic and Maurice van Keulen and Willem Jonker", "conference": "Data Engineering Workshops, 2006. Proceedings. 22nd International Conference on", "pages": "45-45", "publisher": "IEEE", "abstract": "Schema matching attempts to discover semantic mappings between elements of two schemas. Elements are cross compared using various heuristics (e.g., name, data-type, and structure similarity). Seen from a broader perspective, the schema matching problem is a combinatorial problem with an exponential complexity. This makes the naive matching algorithms for large schemas prohibitively inefficient. In this paper we propose a clustering based technique for improving the efficiency of large scale schema matching. The technique inserts clustering as an intermediate step into existing schema matching algorithms. Clustering partitions schemas and reduces the overall matching load, and creates a possibility to trade between the efficiency and effectiveness. The technique can be used in addition to other optimization techniques. In the paper we describe the technique, validate the performance of one \u2026"}}, {"bib": {"title": "Formalizing the XML schema matching problem as a constraint optimization problem", "pub_year": 2005, "author": "Marko Smiljani\u0107 and Maurice Van Keulen and Willem Jonker", "conference": "International Conference on Database and Expert Systems Applications", "pages": "333-342", "publisher": "Springer Berlin Heidelberg", "abstract": "The first step in finding an efficient way to solve any difficult problem is making a complete, possibly formal, problem specification. This paper introduces a formal specification for the problem of semantic XML schema matching. Semantic schema matching has been extensively researched, and many matching systems have been developed. However, formal specifications of problems being solved by these systems do not exist, or are partial. In this paper, we analyze the problem of semantic schema matching, identify its main components and deliver a formal specification based on the constraint optimization problem formalism. Throughout the paper, we consider the schema matching problem as encountered in the context of a large scale XML schema matching application."}}, {"bib": {"title": "Duplicate detection in probabilistic data", "pub_year": 2010, "author": "Fabian Panse and Maurice Van Keulen and Ander De Keijzer and Norbert Ritter", "conference": "Data Engineering Workshops (ICDEW), 2010 IEEE 26th International Conference on", "pages": "179-182", "publisher": "IEEE", "abstract": "Collected data often contains uncertainties. Probabilistic databases have been proposed to manage uncertain data. To combine data from multiple autonomous probabilistic databases, an integration of probabilistic data has to be performed. Until now, however, data integration approaches have focused on the integration of certain source data (relational or XML). There is no work on the integration of uncertain source data so far. In this paper, we present a first step towards a concise consolidation of probabilistic data. We focus on duplicate detection as a representative and essential step in an integration process. We present techniques for identifying multiple probabilistic representations of the same real-world entities."}}, {"bib": {"title": "Unsupervised improvement of named entity extraction in short informal context using disambiguation clues", "pub_year": 2012, "author": "Mena B Habib and Maurice van Keulen", "volume": "925", "pages": "1-10", "publisher": "CEUR-WS. org", "abstract": "Short context messages (like tweets and SMS\u2019s) are a potentially rich source of continuously and instantly updated information. Shortness and informality of such messages are challenges for Natural Language Processing tasks. Most efforts done in this direction rely on machine learning techniques which are expensive in terms of data collection and training. In this paper we present an unsupervised Semantic Web-driven approach to improve the extraction process by using clues from the disambiguation process. For extraction we used a simple Knowledge-Base matching technique combined with a clustering-based approach for disambiguation. Experimental results on a self-collected set of tweets (as an example of short context messages) show improvement in extraction results when using unsupervised feedback from the disambiguation process."}}, {"bib": {"title": "Information Extraction for Social Media", "pub_year": 2014, "author": "Mena B Habib and Maurice van Keulen", "journal": "SWAIE 2014", "pages": "9"}}, {"bib": {"title": "Managing uncertainty: The road towards better data interoperability", "pub_year": 2012, "author": "Maurice van Keulen", "journal": "it-Information Technology Methoden und innovative Anwendungen der Informatik und Informationstechnik", "volume": "54", "number": "3", "pages": "138-146", "abstract": "Data interoperability encompasses the many data management activities needed for      effective information management in anyone\u00b4s or any organization\u00b4s everyday work      such as data cleaning, coupling, fusion, mapping, and information extraction. It      is our conviction that a significant amount of money and time in IT that is      devoted to these activities, is about dealing with one problem: \u201csemantic      uncertainty\u201d. Sometimes data is subjective, incomplete, not current, or      incorrect, sometimes it can be interpreted in different ways, etc. In our      opinion, clean correct data is only a special case, hence data management      technology should treat data quality problems as a fact of life, not as      something to be repaired afterwards. Recent approaches treat uncertainty as an      additional source of information which should be preserved to reduce its impact.      We believe that the road towards better data \u2026"}}, {"bib": {"title": "Automated semantic trajectory annotation with indoor point-of-interest visits in urban areas", "pub_year": 2016, "author": "Victor de Graaff and Rolf A de By and Maurice van Keulen", "conference": "Proceedings of the 31st Annual ACM Symposium on Applied Computing", "pages": "552-559", "publisher": "ACM", "abstract": "User trajectories contain a wealth of implicit information. The places that people visit, provide us with information about their preferences and needs. Furthermore, it provides us with information about the popularity of places, for example at which time of the year or day these places are frequently visited. The potential for behavioral analysis of trajectories is widely discussed in literature, but all of these methods need a pre-processing step: the geometric trajectory data needs to be transformed into a semantic collection or sequence of visited points-of-interest that is more suitable for data mining. Especially indoor activities in urban areas are challenging to detect from raw trajectory data. In this paper, we propose a new algorithm for the automated detection of visited points-of-interest. This algorithm extracts the actual visited points-of-interest well, both in terms of precision and recall, even for the challenging urban \u2026"}}, {"bib": {"title": "Improving toponym disambiguation by iteratively enhancing certainty of extraction", "pub_year": 2012, "author": "Mena Badieh Habib and Maurice van Keulen", "publisher": "SciTePress", "abstract": "Named entity extraction (NEE) and disambiguation (NED) have received much attention in recent years. Typical fields addressing these topics are information retrieval, natural language processing, and semantic web. This paper addresses two problems with toponym extraction and disambiguation (as a representative example of named entities). First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disambiguation techniques mostly take as input extracted named entities without considering the uncertainty and imperfection of the extraction process. It is the aim of this paper to investigate both avenues and to show that explicit handling of the uncertainty of annotation has much potential for making both extraction and disambiguation more robust. We conducted experiments with a set of holiday home descriptions with the aim to extract and disambiguate toponyms. We show that the extraction confidence probabilities are useful in enhancing the effectiveness of disambiguation. Reciprocally, retraining the extraction models with information automatically derived from the disambiguation results, improves the extraction models. This mutual reinforcement is shown to even have an effect after several automatic iterations."}}, {"bib": {"title": "Storing and querying probabilistic XML using a probabilistic relational DBMS", "pub_year": 2010, "author": "Emiel S Hollander and M van Keulen", "publisher": "Centre for Telematics and Information Technology", "abstract": "This work explores the feasibility of storing and querying probabilistic XML in a probabilistic relational database. Our approach is to adapt known techniques for mapping XML to relational data such that the possible worlds are preserved. We show that this approach can work for any XML-to-relational technique by adapting a representative schema-based (inlining) as well as a representative schemaless technique (XPath Accelerator). We investigate the maturity of probabilistic relational databases for this task with experiments with one of the state-ofthe-art systems, called Trio."}}, {"bib": {"title": "Quality measures in uncertain data management", "pub_year": 2007, "author": "Ander de Keijzer and Maurice van Keulen", "conference": "International Conference on Scalable Uncertainty Management", "pages": "104-115", "publisher": "Springer Berlin Heidelberg", "abstract": "Many applications deal with data that is uncertain. Some examples are applications dealing with sensor information, data integration applications and healthcare applications. Instead of these applications having to deal with the uncertainty, it should be the responsibility of the DBMS to manage all data including uncertain data. Several projects do research on this topic. In this paper, we introduce four measures to be used to assess and compare important characteristics of data and systems: uncertainty density, answer decisiveness and adapted precision and recall measures."}}, {"bib": {"title": "Deep Physiological Arousal Detection in a Driving Simulator using Wearable Sensors", "pub_year": 2017, "author": "Aaqib Saeed and Stojan Trajanovski and Maurice van Keulen and Jan van Erp", "journal": "IEEE International Conference on Data Miningworkshop: Data Mining in Biomedical Informatics and Healthcare (DMBIH)", "abstract": "Driving is an activity that requires considerable alertness. Insufficient attention, imperfect perception, inadequate information processing, and sub-optimal arousal are possible causes of poor human performance. Understanding of these causes and the implementation of effective remedies is of key importance to increase traffic safety and improve driver's well-being. For this purpose, we used deep learning algorithms to detect arousal level, namely, under-aroused, normal and over-aroused for professional truck drivers in a simulated environment. The physiological signals are collected from 11 participants by wrist wearable devices. We presented a cost effective ground-truth generation scheme for arousal based on a subjective measure of sleepiness and score of stress stimuli. On this dataset, we evaluated a range of deep neural network models for representation learning as an alternative to handcrafted feature \u2026"}}, {"bib": {"title": "An injection with tree awareness: adding staircase join to postgreSQL", "pub_year": 2004, "author": "Sabine Mayer and Torsten Grust and Maurice van Keulen and Jens Teubner", "conference": "Proceedings of the Thirtieth international conference on Very large data bases-Volume 30", "pages": "1305-1308", "publisher": "VLDB Endowment", "abstract": "The syntactic wellformedness constraints of XML (opening and closing tags nest properly) imply that XML processors face the challenge to effciently handle data that takes the shape of ordered, unranked trees. Although RDBMSs have originally been designed to manage table-shaped data, we propose their use as XML and XPath processors. In our setup, the database system employs a relational XML document encoding, the XPath accelerator [1], which maps information about the XML node hierarchy to a table, thus making it possible to evaluate XPath expressions on SQL hosts. Conventional RDBMSs, nevertheless, remain ignorant of many interesting properties of the encoded tree data, and were thus found to make no or poor use of these properties. This is why we devised a new join algorithm, the staircase join [2], which incorporates the tree-specific knowledge required for an effcient SQL-based evaluation of XPath expressions. In a sense, this demonstration delivers the promise we have made at VLDB 2003 [2]: a notion of tree awareness can be injected into a conventional disk-based RDBMS kernel in terms of staircase join. The demonstration features a side-by-side comparison of both, an original and a staircase-join enhanced instance of PostgreSQL [4]. The required changes to PostgreSQL were local, the achieved effect, however, is significant: the demonstration proves that this injection of tree awareness turns PostgreSQL into a high-performance XML processor that closely adheres to the XPath semantics."}}, {"bib": {"title": "Deep web entity monitoring", "pub_year": 2013, "author": "Mohamamdreza Khelghati and Djoerd Hiemstra and Maurice Van Keulen", "conference": "Proceedings of the 22nd International Conference on World Wide Web", "pages": "377-382", "publisher": "ACM", "abstract": "Accessing information is an essential factor in decision making processes occurring in different domains. Therefore, broadening the coverage of available information for the decision makers is of a vital importance. In such a informationthirsty environment, accessing every source of information is considered highly valuable. Nowadays, the main or the most general approach for finding and accessing information sources is searching queries over general search engines such as Google, Yahoo, or Bing. However, these search engines do not cover all the data available on the Web. In addition to the fact that none of these search engines cover all the webpages existing on the Web, they miss the data behind web search forms. This data is defined as hidden web or deep web which is not accessible through search engines. It is estimated that deep web contains data in a scale several times bigger than the data \u2026"}}, {"bib": {"title": "Indeterministic handling of uncertain decisions in deduplication", "pub_year": 2013, "author": "Fabian Panse and Maurice van Keulen and Norbert Ritter", "journal": "Journal of Data and Information Quality (JDIQ)", "volume": "4", "number": "2", "pages": "1-25", "publisher": "ACM", "abstract": "In current research and practice, deduplication is usually considered as a deterministic approach in which database tuples are either declared to be duplicates or not. In ambiguous situations, however, it is often not completely clear-cut, which tuples represent the same real-world entity. In deterministic approaches, many realistic possibilities may be ignored, which in turn can lead to false decisions. In this article, we present an indeterministic approach for deduplication by using a probabilistic target model including techniques for proper probabilistic interpretation of similarity matching results. Thus, instead of deciding for one of the most likely situations, all realistic situations are modeled in the resultant data. This approach minimizes the negative impact of false decisions. Moreover, the deduplication process becomes almost fully automatic and human effort can be largely reduced. To increase applicability, we \u2026"}}, {"bib": {"title": "Named Entity Extraction and Disambiguation: The Reinforcement Effect.", "pub_year": 2011, "author": "Mena Badieh Habib and Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology (CTIT)", "abstract": "Named entity extraction and disambiguation have received much attention in recent years. Typical fields addressing these topics are information retrieval, natural language processing, and semantic web. Although these topics are highly dependent, almost no existing works examine this dependency. It is the aim of this paper to examine the dependency and show how one affects the other, and vice versa. We conducted experiments with a set of descriptions of holiday homes with the aim to extract and disambiguate toponyms as a representative example of named entities. We experimented with three approaches for disambiguation with the purpose to infer the country of the holiday home. We examined how the effectiveness of extraction influences the effectiveness of disambiguation, and reciprocally, how filtering out ambiguous names (an activity that depends on the disambiguation process) improves the effectiveness of extraction. Since this, in turn, may improve the effectiveness of disambiguation again, it shows that extraction and disambiguation may reinforce each other."}}, {"bib": {"title": "TwitterNEED: A hybrid approach for named entity extraction and disambiguation for tweet", "pub_year": 2015, "author": "Mena B Habib and Maurice van Keulen", "journal": "Natural Language Engineering", "pages": "1-34", "publisher": "Cambridge University Press", "abstract": "Twitter is a rich source of continuously and instantly updated information. Shortness and informality of tweets are challenges for Natural Language Processing tasks. In this paper, we present TwitterNEED, a hybrid approach for Named Entity Extraction and Named Entity Disambiguation for tweets. We believe that disambiguation can help to improve the extraction process. This mimics the way humans understand language and reduces error propagation in the whole system. Our extraction approach aims for high extraction recall first, after which a Support Vector Machine attempts to filter out false positives among the extracted candidates using features derived from the disambiguation phase in addition to other word shape and Knowledge Base features. For Named Entity Disambiguation, we obtain a list of entity candidates from the YAGO Knowledge Base in addition to top-ranked pages from the Google search \u2026"}}, {"bib": {"title": "Concept extraction challenge: University of twente at# msm2013", "pub_year": 2013, "author": "Mena B Habib and Maurice van Keulen and Zhemin Zhu", "publisher": "CEUR", "abstract": "Twitter messages are a potentially rich source of continuously and instantly updated information. Shortness and informality of such messages are challenges for Natural Language Processing tasks. In this paper we present a hybrid approach for Named Entity Extraction (NEE) and Classification (NEC) for tweets. The system uses the power of the Conditional Random Fields (CRF) and the Support Vector Machines (SVM) in a hybrid way to achieve better results. For named entity type classification we use AIDA [8] disambiguation system to disambiguate the extracted named entities and hence find their type."}}, {"bib": {"title": "A possible world approach to uncertain relational data", "pub_year": 2004, "author": "Ander De Keijzer and Maurice Van Keulen", "conference": "Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.", "pages": "922-926", "publisher": "IEEE", "abstract": "Data exchange between embedded systems and other small or large computing device increases. Since data in different data sources may refer to the same real world objects, data cannot simply be merged. Furthermore, in many situations, conflicts in data about the same real world objects need to be resolved without interference from a user. We report on an attempt to make a RDBMS probabilistic, i.e., data in a relation represents all possible views on the real world, in order to achieve unattended data integration. We define a probabilistic relational data model and review standard SQL query primitives in the light of probabilistic data. It appears that thinking in terms of 'possible worlds' is powerful in determining the proper semantics of these query primitives."}}, {"bib": {"title": "Point of interest to region of interest conversion", "pub_year": 2013, "author": "Victor de Graaff and Rolf A de By and Maurice van Keulen and Jan Flokstra", "conference": "Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems", "pages": "388-391", "publisher": "ACM", "abstract": "Trajectories of people contain a vast amount of information on users' interests and popularity of locations. To obtain this information, the places visited by the owner of the device on such a trajectory need to be recognized. However, the location information on a point of interest (POI) in a database is normally limited to an address and a coordinate pair, rather than a polygon describing its boundaries. A region of interest can be used to intersect trajectories to match trajectories with objects of interest. In the absence of expensive and often not publicly available detailed spatial data like cadastral data, we need to approximate this ROI. In this paper, we present several approaches to approximate the size and shape of ROIs, by integrating data from multiple public sources, a validation technique, and a validation of these approaches against the cadastral data of the city of Enschede, The Netherlands."}}, {"bib": {"title": "A generic open world named entity disambiguation approach for tweets", "pub_year": 2013, "author": "Mena B Habib and M van Keulen", "publisher": "SciTePress", "abstract": "Social media is a rich source of information. To make use of this information, it is sometimes required to extract and disambiguate named entities. In this paper, we focus on named entity disambiguation (NED) in twitter messages. NED in tweets is challenging in two ways. First, the limited length of Tweet makes it hard to have enough context while many disambiguation techniques depend on it. The second is that many named entities in tweets do not exist in a knowledge base (KB). We share ideas from information retrieval (IR) and NED to propose solutions for both challenges. For the first problem we make use of the gregarious nature of tweets to get enough context needed for disambiguation. For the second problem we look for an alternative home page if there is no Wikipedia page represents the entity. Given a mention, we obtain a list of Wikipedia candidates from YAGO KB in addition to top ranked pages from Google search engine. We use Support Vector Machine (SVM) to rank the candidate pages to find the best representative entities. Experiments conducted on two data sets show better disambiguation results compared with the baselines and a competitor."}}, {"bib": {"title": "IMPrECISE: Good-is-good-enough Data Integration", "pub_year": 2009, "author": "Maurice van Keulen and C Koch and B K\u00f6nig-Ries and V Markl and M van Keulen", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik", "abstract": "The IMPrECISE system is a probabilistic XML database system which supports near-automatic integration of XML documents. What is required of the user is to configure the system with a few simple knowledge rules allowing the system to sufficiently eliminate nonsense possibilities. We demonstrate the integration process under conditions with varying degrees of confusion and different sets of rules."}}, {"bib": {"title": "IMPrECISE: Good-is-good-enough data integration", "pub_year": 2008, "author": "Ander de Keijzer and Maurice van Keulen", "conference": "Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on", "pages": "1548-1551", "publisher": "IEEE", "abstract": "IMPrECISE is an XQuery module that adds probabilistic XML functionality to an existing XML DBMS, in our case MonetDB/XQuery. We demonstrate probabilistic XML and data integration functionality of IMPrECISE. The prototype is configurable with domain knowledge such that the amount of uncertainty arising during data integration is reduced to an acceptable level, thus obtaining a \"good is good enough\" data integration with minimal human effort."}}, {"bib": {"title": "Taming data explosion in probabilistic information integration", "pub_year": 2006, "author": "Ander De Keijzer and Maurice Van Keulen and Yiping Li", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "Data integration has been a challenging problem for decades. In an ambient environment, where many autonomous devices have their own information sources and network connectivity is ad hoc and peer-topeer, it even becomes a serious bottleneck. To enable devices to exchange information without the need for interaction with a user at data integration time and without the need for extensive semantic annotations, a probabilistic approach seems rather promising. It simply teaches the device how to cope with the uncertainty occurring during data integration. Unfortunately, without any kind of world knowledge, almost everything becomes uncertain, hence maintaining all possibilities produces huge integrated information sources. In this paper, we claim that only very simple and generic rules are enough world knowledge to drastically reduce the amount of uncertainty, hence to tame the data explosion to a manageable size."}}, {"bib": {"title": "A hybrid approach for robust multilingual toponym extraction and disambiguation", "pub_year": 2013, "author": "Mena B Habib and Maurice van Keulen", "conference": "Intelligent Information Systems Symposium", "pages": "1-15", "publisher": "Springer, Berlin, Heidelberg", "abstract": "Toponym extraction and disambiguation are key topics recently addressed by fields of Information Extraction and Geographical Information Retrieval. Toponym extraction and disambiguation are highly dependent processes. Not only toponym extraction effectiveness affects disambiguation, but also disambiguation results may help improving extraction accuracy. In this paper we propose a hybrid toponym extraction approach based on Hidden Markov Models (HMM) and Support Vector Machines (SVM). Hidden Markov Model is used for extraction with high recall and low precision. Then SVM is used to find false positives based on informativeness features and coherence features derived from the disambiguation results. Experimental results conducted with a set of descriptions of holiday homes with the aim to extract and disambiguate toponyms showed that the proposed approach outperform the state of the \u2026"}}, {"bib": {"title": "Size estimation of non-cooperative data collections", "pub_year": 2012, "author": "Mohammadreza Khelghati and Djoerd Hiemstra and Maurice van Keulen", "conference": "Proceedings of the 14th International Conference on Information Integration and Web-based Applications & Services", "pages": "239-246", "publisher": "ACM", "abstract": "With the increasing amount of data in deep web sources (hidden from general search engines behind web forms), accessing this data has gained more attention. In the algorithms applied for this purpose, it is the knowledge of a data source size that enables the algorithms to make accurate decisions in stopping the crawling or sampling processes which can be so costly in some cases [14]. This tendency to know the sizes of data sources is increased by the competition among businesses on the Web in which the data coverage is critical. In the context of quality assessment of search engines [7], search engine selection in the federated search engines, and in the resource/collection selection in the distributed search field [19], this information is also helpful. In addition, it can give an insight over some useful statistics for public sectors like governments. In any of these mentioned scenarios, in the case of facing a non \u2026"}}, {"bib": {"title": "Towards geosocial recommender systems", "pub_year": 2012, "author": "Victor de Graaff and Maurice van Keulen and Rolf A de By", "pages": "1-4", "abstract": "The usage of social networks sites (SNSs), such as Facebook, and geosocial networks (GSNs), such as Foursquare, has increased tremendously over the past years. The willingness of users to share their current locations and experiences facilitate the creation of geographical recommender systems based on user generated content (UGC). This idea has been used to create a substantial amount of geosocial recommender systems (GRSs), such as Gogobot, TripIt, and Trippy already, but can be applied to more complex scenarios, such as the recommendation of products with a strong binding to their region, such as real estate or vacation destinations."}}, {"bib": {"title": "Analysis of the NIST database towards the composition of vulnerabilities in attack scenarios", "pub_year": 2008, "author": "Virginia NL Franqueira and Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology, University of Twente", "abstract": "The composition of vulnerabilities in attack scenarios has been traditionally performed based on detailed pre- and post-conditions. Although very precise, this approach is dependent on human analysis, is time consuming, and not at all scalable. We investigate the NIST National Vulnerability Database (NVD) with three goals: (i) understand the associations among vulnerability attributes related to impact, exploitability, privilege, type of vulnerability and clues derived from plaintext descriptions, (ii) validate our initial composition model which is based on required access and resulting effect, and (iii) investigate the maturity of XML database technology for performing statistical analyses like this directly on the XML data. In this report, we analyse 27,273 vulnerability entries (CVE [1]) from the NVD. Using only nominal information, we are able to e.g. identify clusters in the class of vulnerabilities with no privilege which represent 52% of the entries."}}, {"bib": {"title": "MonetDB/XQuery\u2014consistent and efficient updates on the pre/Post plane", "pub_year": 2006, "author": "Peter Boncz and Jan Flokstra and Torsten Grust and Maurice van Keulen and Stefan Manegold and Sjoerd Mullender and Jan Rittinger and Jens Teubner", "conference": "International Conference on Extending Database Technology", "pages": "1190-1193", "publisher": "Springer Berlin Heidelberg", "abstract": "Relational XQuery processors aim at leveraging mature relational DBMS query processing technology to provide scalability and efficiency. To achieve this goal, various storage schemes have been proposed to encode the tree structure of XML documents in flat relational tables. Basically, two classes can be identified: (1) encodings using fixed-length surrogates, like the preorder ranks in the pre/post encoding [5] or the equivalent pre/size/level encoding [8], and (2) encodings using variable-length surrogates, like, e.g., ORDPATH [9] or P-PBiTree [12]. Recent research [1] showed a clear advantage of the former for efficient evaluation of XPath location steps, exploiting techniques like cheap node order tests, positional lookup, and node skipping in staircase join [7]. However, once updates are involved, variable-length surrogates are often considered the better choice, mainly as a straightforward \u2026"}}, {"bib": {"title": "The IMPRESS DDT: A database design toolbox based on a formal specification language", "pub_year": 1994, "author": "Jan Flokstra and Maurice Van Keulen and Jacek Skowronek", "pages": "506", "abstract": "The Database Design Tool prototype is being developed in the IMPRESS project (Esprit project 6355). The IMPRESS project started in May 1992 and aims at creating a lowlevel storage manager tailored for multimedia applications, together with a library of efficient operators, a programming environment, high-level design tools and methodology. The DDT is part of this last effort.The project focuses on the field of Technical Information Systems, where there is aneed for tools supporting modeling of complex objects. Designers in this field usually use incremental design or step by step prototyping, because this seems to be best suited for users coping with complexity and uncertainty about their own needs or requirements. The IMPRESS DDT aims at supporting the database design part of this process."}}, {"bib": {"title": "Need4tweet: a twitterbot for tweets named entity extraction and disambiguation", "pub_year": 2015, "author": "Mena Habib and Maurice van Keulen", "journal": "Proceedings of ACL-IJCNLP 2015 System Demonstrations", "pages": "31-36", "abstract": "In this demo paper, we present NEED4Tweet, a Twitterbot for named entity extraction (NEE) and disambiguation (NED) for Tweets. The straightforward application of state-of-the-art extraction and disambiguation approaches on informal text widely used in Tweets, typically results in significantly degraded performance due to the lack of formal structure; the lack of sufficient context required; and the seldom entities involved. In this paper, we introduce a novel framework that copes with the introduced challenges. We rely on contextual and semantic features more than syntactic features which are less informative. We believe that disambiguation can help to improve the extraction process. This mimics the way humans understand language."}}, {"bib": {"title": "Distributed XML Database Systems", "pub_year": 2002, "author": "Marko Smiljanic and Henk Blanken and Maurice van Keulen and Willem Jonker", "number": "TR-CTI", "pages": "1-43", "publisher": "University of Twente, Centre for Telematics and", "abstract": "Invention of XML as a universal standard for data representation triggered enormously wide efforts for its adaptation into almost every IT activity. Databases took one of the focusing places of the XML research.This paper investigates the development path of XML from its origins to its current place in distributed database systems. Through enumeration of features and analysis of the problems related to XML and distributed database systems it forms a platform for understanding the consequences of adopting XML to this area of IT."}}, {"bib": {"title": "Moa: extensibility and efficiency in querying nested data", "pub_year": 2002, "author": "Maurice van Keulen and Jochem Vonk and Arjen P Vries and Jan Flokstra and Henk Ernst Blok", "number": "TR-CTI", "publisher": "University of Twente, Centre for Telematics and", "abstract": "Advanced non-traditional application domains such as geographic information systems and digital library systems demand advanced data management support. In an effort to cope with this demand, we present a novel multi-model DBMS architecture which provides efficient evaluation of queries on complexly structured data. A vital role in this architecture is played by the Moa language featuring a nested relational data model based on XNF2, in which we placed renewed interest. Furthermore, the architecture allows extensibility on all of its levels providing the means to better integrate domain-specific algorithms into the system. In addition to this, the extensibility of the Moa language is designed in a way that optimization obstacles due to blackbox treatment of ADTs is avoided. This combination of well-integrated domainspecific algorithms, extensibility open to optimization, and a mapping of queries on complexly \u2026"}}, {"bib": {"title": "Overview of query optimization in XML database systems", "pub_year": 2007, "author": "R Abdel Kader and Maurice van Keulen", "number": "TR-CTI", "publisher": "Centre for Telematics and Information Technology, University of Twente", "abstract": "Overview of query optimization in XML database systems (2007) | www.narcis.nl KNAW \nKNAW Narcis Back to search results University of Twente Publication Overview of query \noptimization in XML database systems (2007) Pagina-navigatie: Main Save publication \nSave as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Overview of \nquery optimization in XML database systems Series CTIT Technical Report Series. Centre \nfor Telematics and Information Technology, University of Twente Author Abdel Kader, R.; \nvan Keulen, Maurice Publisher Databases (Former) Date issued 2007-11-12 Access \nRestricted Access Reference(s) METIS-245765, IR-64449, EWI-11361 Language und Type \nReport Publisher Centre for Telematics and Information Technology (CTIT) Publication https://research.utwente.nl/en/publications/overview-of-quer... \nPersistent Identifier urn:nbn:nl:ui:28-64449 Metadata XML of Go .\u2026"}}, {"bib": {"title": "Probabilistic Data Integration.", "pub_year": 2019, "author": "Maurice Van Keulen", "pages": "1-9", "abstract": "Probabilistic data integration (PDI) is a specific kind of data integration where integration problems such as inconsistency and uncertainty are handled by means of a probabilistic data representation. The approach is based on the view that data quality problems (as they occur in an integration process) can be modeled as uncertainty (van Keulen 2012) and this uncertainty is considered an important result of the integration process (Magnani and Montesi 2010). The PDI process contains two phases (see Figure 1):(i) a quick partial integration where certain data quality problems are not solved immediately, but explicitly represented as uncertainty in the resulting integrated data stored in a probabilistic database;(ii) continuous improvement by using the data\u2014a probabilistic database can be queried directly resulting in possible or approximate answers (Dalvi et al 2009)\u2014and gathering evidence (eg, user feedback) for improving the data quality."}}, {"bib": {"title": "Handling uncertainty in information extraction", "pub_year": 2011, "author": "Maurice van Keulen and Mena B Habib", "conference": "Proceedings of the 7th International Conference on Uncertainty Reasoning for the Semantic Web-Volume 778", "pages": "109-112", "publisher": "CEUR-WS. org"}}, {"bib": {"title": "Probabilistic Data Integration", "pub_year": 2009, "author": "M van Keulen", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik"}}, {"bib": {"title": "User feedback in probabilistic integration", "pub_year": 2007, "author": "Ander de Keijzer and Maurice van Keulen", "conference": "18th International Workshop on Database and Expert Systems Applications (DEXA 2007)", "pages": "377-381", "publisher": "IEEE", "abstract": "Data integration approaches mostly attempt to resolve semantic uncertainty and conflicts between data sources during the data integration process. In some application areas, this is impractical or even prohibitive. We propose a probabilistic XML approach that allows storage and querying of uncertain data. It requires only minimal user involvement during data integration, because most semantic uncertainty and conflicts can be resolved by exploiting user feedback on query results, thus effectively postponing user involvement to query time when a user already interacts with the system. We show that repeated feedback gradually improves the factual correctness of integrated data."}}, {"bib": {"title": "Post-structuring radiology reports of breast cancer patients for clinical quality assurance", "pub_year": 2019, "author": "Shreyasi Pathak and Jorit van Rossen and Onno Vijlbrief and Jeroen Geerdink and Christin Seifert and Maurice van Keulen", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "volume": "17", "number": "6", "pages": "1883-1894", "publisher": "IEEE", "abstract": "Hospitals often set protocols based on well defined standards to maintain the quality of patient reports. To ensure that the clinicians conform to the protocols, quality assurance of these reports is needed. Patient reports are currently written in free-text format, which complicates the task of quality assurance. In this paper, we present a machine learning based natural language processing system for automatic quality assurance of radiology reports on breast cancer. This is achieved in three steps: we i) identify the top-level structure (headings) of the report, ii) classify the report content into the top-level headings, and iii) convert the free-text detailed findings in the report to a semi-structured format (post-structuring). Top level structure and content of report were predicted with an F1 score of 0.97 and 0.94, respectively, using Support Vector Machine (SVM) classifiers. For automatic structuring, our proposed hierarchical \u2026"}}, {"bib": {"title": "Comparing Process Models for Patient Populations: Application in Breast Cancer Care", "pub_year": 2019, "author": "Francesca Marazza and Faiza Allah Bukhsh and Onno Vijlbrief and Jeroen Geerdink and Shreyasi Pathak and Maurice van Keulen and Christin Seifert", "conference": "International Workshop Process-Oriented Data Science for Healthcare 2019", "abstract": "Processes in organisations such as hospitals, may deviate from intended standard processes, due to unforeseeable events and the complexity of the organisation. For hospitals, the knowledge of actual patient streams for patient populations (e.g., severe or non-severe cases) is important for quality control and improvement. Process discovery from event data in electronic health records can shed light on the patient flows, but their comparison for different populations is cumbersome and time-consuming. In this paper, we present an approach for the automatic comparison of process models extracted from events in electronic health records. Concretely, we propose to compare processes for different patient populations by cross-log conformance checking, and standard graph similarity measures obtained from the directed graph underlying the process model. Results from a case study on breast cancer care \u2026"}}, {"bib": {"title": "Automatic Structuring of Breast Cancer Radiology Reports for Quality Assurance", "pub_year": 2018, "author": "Shreyasi Pathak and Jorit van Rossen and Onno Vijlbrief and Jeroen Geerdink and Christin Seifert and Maurice van Keulen", "conference": "6th Workshop on Data Mining in Biomedical Informatics and Healthcare 2018", "abstract": "Hospitals often set protocols based on well defined standards to maintain quality of patient reports. To ensure that the clinicians conform to the protocols, quality assurance of these reports is needed. Patient reports are currently written in free-text format, which complicates the task of quality assurance. In this paper, we present a machine learning based natural language processing system for automatic quality assurance of radiology reports on breast cancer. This is achieved in three steps: we i) identify the top level structure of the report, ii) check whether the information under each section corresponds to the section heading, iii) convert the free-text detailed findings in the report to a semi-structured format. Top level structure and content of report were predicted with an F1 score of 0.97 and 0.94 respectively using Support Vector Machine (SVM). For automatic structuring, our proposed hierarchical Conditional Random Field (CRF) outperformed the baseline CRF with an F1 score of 0.78 vs 0.71. The third step generates a semi-structured XML format of the free-text report, which helps to easily visualize the conformance of the findings to the protocols. This format also allows easy extraction of specific information for other purposes such as search, evaluation and research."}}, {"bib": {"title": "Uncertain Groupings: Probabilistic combination of grouping data", "pub_year": 2014, "author": "Brend Wanders and Maurice van Keulen and PE van der Vet", "publisher": "Centre for Telematics and Information Technology, University of Twente", "abstract": "Probabilistic approaches for data integration have much potential [7]. We view data integration as an iterative process where data understanding gradually increases as the data scientist continuously refines his view on how to deal with learned intricacies like data conflicts. This paper presents a probabilistic approach for integrating data on groupings. We focus on a bio-informatics use case concerning homology. A bio-informatician has a large number of homology data sources to choose from. To enable querying combined knowledge contained in these sources, they need to be integrated. We validate our approach by integrating three real-world biological databases on homology in three iterations."}}, {"bib": {"title": "Neogeography: The challenge of channelling large and ill-behaved data streams", "pub_year": 2011, "author": "Mena B Habib and Peter Apers and Maurice van Keulen", "conference": "2011 IEEE 27th International Conference on Data Engineering Workshops", "pages": "284-287", "publisher": "IEEE", "abstract": "Neogeography is the combination of user generated data and experiences with mapping technologies. In this paper we present a research project to extract valuable structured information with a geographic component from unstructured user generated text in wikis, forums, or SMSes. The project intends to help workers communities in developing countries to share their knowledge, providing a simple and cheap way to contribute and get benefit using the available communication technology."}}, {"bib": {"title": "The TM Manual; version 2.0, revision e", "pub_year": 1995, "author": "Ren\u00e9 Bal and Herman Balsters and RA de By and Alexander Bosschaart and Jan Flokstra and Maurice van Keulen and J Skowronek and B Termorshuizen", "publisher": "Universiteit Twente, Technical report IMPRESS/UT-TECH-T79-001-R2, Enschede, The Netherlands", "abstract": "This is the TM language manual, describing version 2.0 of the database specification language TM. This document is a working document, which means that not all (although many) of our ideas concerning the construction of the language have been thoroughly evaluated. Most importantly, we lack a fine set of application case studies, which would undoubtedly result in further enhancements and tuning of the language. This is obviously a cyclic problem, because how can we obtain good case studies if there is no manual? Thus, the main goal of this document is to get some people out in the field of database specification use TM, and report on their findings.We use TM as a high-level language for the design and specification of object-oriented database schemas in an efficient and effective manner. The TM language and its accompanying design tools enable users to perform complex semantical analyses of schemas, thus paving the way to a complete debugging of the conceptual design. As a design language, TM is equipped with powerful structuring primitives which enable a user to arrive at natural and intuitively correct designs. These structuring primitives are characterized by the following features"}}, {"bib": {"title": "Revisiting the formal foundation of probabilistic databases", "pub_year": 2015, "author": "Brend Wanders and Maurice van Keulen", "publisher": "Atlantis Press", "abstract": "One of the core problems in soft computing is dealing with uncertainty in data. In this paper, we revisit the formal foundation of a class of probabilistic databases with the purpose to (1) obtain data model independence,(2) separate metadata on uncertainty and probabilities from the raw data,(3) better understand aggregation, and (4) create more opportunities for optimization. The paper presents the formal framework and validates data model independence by showing how to a obtain probabilistic Datalog as well as a probabilistic relational algebra by applying the framework to their non-probabilistic counterparts. We conclude with a discussion on the latter three goals."}}, {"bib": {"title": "Named Entity Extraction and Linking Challenge: University of Twente at# Microposts2014", "pub_year": 2014, "author": "Mena B Habib and Maurice van Keulen and Zhemin Zhu", "publisher": "CEUR Workshop Proceedings", "abstract": "Twitter is a potentially rich source of continuously and instantly updated information. Shortness and informality of tweets are challenges for Natural Language Processing (NLP) tasks. In this paper we present a hybrid approach for Named Entity Extraction (NEE) and Linking (NEL) for tweets. Although NEE and NEL are two topics that are well studied in literature, almost all approaches treated the two problems separately. We believe that disambiguation (linking) could help improving the extraction process. We call this potential for mutual improvement, the reinforcement effect. It mimics the way humans understand natural language. Furthermore, our proposed approaches handles uncertainties involved in the two processes by considering possible alternatives."}}, {"bib": {"title": "To use or not to use: guidelines for researchers using data from online social networking sites", "pub_year": 2013, "author": "Aimee van Wynsberghe and Henry Been and Maurice van Keulen", "publisher": "RRI, Rict Responsible Innovation"}}, {"bib": {"title": "Moa and the multi-model architecture: a new perspective on NF2", "pub_year": 2003, "author": "Maurice van Keulen and Jochem Vonk and Arjen P de Vries and Jan Flokstra and Henk Ernst Blok", "conference": "International Conference on Database and Expert Systems Applications", "pages": "67-76", "publisher": "Springer Berlin Heidelberg", "abstract": "Advanced non-traditional application domains such as geographic information systems and digital library systems demand advanced data management support. In an effort to cope with this demand, we present the concept of a novel multi-model DBMS architecture which provides evaluation of queries on complexly structured data without sacrificing efficiency. A vital role in this architecture is played by the Moa language featuring a nested relational data model based on XNF 2, in which we placed renewed interest. Furthermore, extensibility in Moa avoids optimization obstacles due to black-box treatment of ADTs. The combination of a mapping of queries on complexly structured data to an efficient physical algebra expression via a nested relational algebra, extensibility open to optimization, and the consequently better integration of domain-specific algorithms, makes that the Moa system can \u2026"}}, {"bib": {"title": "Detecting Hacked Twitter Accounts based on Behavioural Change", "pub_year": 2017, "author": "Meike Nauta and Mena Badieh Habib and Maurice van Keulen", "conference": "13th International Conference on Web Information Systems and Technologies, WEBIST 2017", "publisher": "INSTICC Institute for Systems and Technologies of Information, Control and Communication", "abstract": "Social media accounts are valuable for hackers for spreading phishing links, malware and spam. Furthermore, some people deliberately hack an acquaintance to damage his or her image. This paper describes a classification for detecting hacked Twitter accounts. The model is mainly based on features associated with behavioural change such as changes in language, source, URLs, retweets, frequency and time. We experiment with a Twitter data set containing tweets of more than 100 Dutch users including 37 who were hacked. The model detects 99% of the malicious tweets which proves that behavioural changes can reveal a hack and that anomaly-based features perform better than regular features. Our approach can be used by social media systems such as Twitter to automatically detect a hack of an account only a short time after the fact allowing the legitimate owner of the account to be warned or protected, preventing reputational damage and annoyance."}}, {"bib": {"title": "JudgeD: a Probabilistic Datalog with Dependencies", "pub_year": 2016, "author": "B Wanders and M van Keulen and Jan Flokstra", "publisher": "AAAI Press", "abstract": "We present JudgeD, a probabilistic datalog. A JudgeD program defines a distribution over a set of traditional datalog programs by attaching logical sentences to clauses to implicitly specify traditional data programs. Through the logical sentences, JudgeD provides a novel method for the expression of complex dependencies between both rules and facts. JudgeD is implemented as a proof-of-concept in the language Python. The implementation allows connection to external data sources, and features both a Monte Carlo probability approximation as well as an exact solver supported by BDDs. Several directions for future work are discussed and the implementation is released under the MIT license."}}, {"bib": {"title": "Generic knowledge-based analysis of social media for recommendations", "pub_year": 2015, "author": "Victor de Graaff and Anne van de Venis and Maurice van Keulen and RA de By", "publisher": "ACM", "abstract": "Recommender systems have been around for decades to help people find the best matching item in a pre-defined item set. Knowledge-based recommender systems are used to match users based on information that links the two, but they often focus on a single, specific application, such as movies to watch or music to listen to. In this paper, we present our Interest-Based Recommender System (IBRS). This knowledge-based recommender system provides recommendations that are generic in three dimensions: IBRS is (1) domain-independent,(2) language-independent, and (3) independent of the used social medium. To match user interests with items, the first are derived from the user\u2019s social media profile, enriched with a deeper semantic embedding obtained from the generic knowledge base DBpedia. These interests are used to extract personalized recommendations from a tagged item set from any domain, in any language. We also present the results of a validation of IBRS by a test user group of 44 people using two item sets from separate domains: greeting cards and holiday homes."}}, {"bib": {"title": "Information extraction, data integration, and uncertain data management: The state of the art", "pub_year": 2011, "author": "Mena B Habib and Maurice van Keulen", "journal": "CTIT Technical Report Series", "number": "TR-CTIT-11-06", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "Information Extraction, data Integration, and uncertain data management are different areas of research that got vast focus in the last two decades. Many researches tackled those areas of research individually. However, information extraction systems should have integrated with data integration methods to make use of the extracted information. Handling uncertainty in extraction and integration process is an important issue to enhance the quality of the data in such integrated systems. This article presents the state of the art of the mentioned areas of research and shows the common grounds and how to integrate information extraction and data integration under uncertainty management cover."}}, {"bib": {"title": "Rox: The robustness of a run-time xquery optimizer against correlated data", "pub_year": 2010, "author": "Riham Abdel Kader and Peter Boncz and Stefan Manegold and Maurice van Keulen", "conference": "Data Engineering (ICDE), 2010 IEEE 26th International Conference on", "pages": "1185-1188", "publisher": "IEEE", "abstract": "We demonstrate ROX, a run-time optimizer of XQueries, that focuses on finding the best execution order of XPath steps and relational joins in an XQuery. The problem of join ordering has been extensively researched, but the proposed techniques are still unsatisfying. These either rely on a cost model which might result in inaccurate estimations, or explore only a restrictive number of plans from the search space. ROX is developed to tackle these problems. ROX does not need any cost model, and defers query optimization to run-time intertwining optimization and execution steps. In every optimization step, sampling techniques are used to estimate the cardinality of unexecuted steps and joins to make a decision which sequence of operators to process next. Consequently, each execution step will provide updated and accurate knowledge about intermediate results, which will be used during the next optimization \u2026"}}, {"bib": {"title": "Loop-lifted staircase join: from XPath to XQuery", "pub_year": 2005, "author": "PA Boncz and Torsten Grust and M van Keulen and Stefan Manegold and Jan Rittinger and Jens Teubner", "journal": "Information Systems [INS]", "number": "E 0510", "pages": "1-17", "publisher": "CWI", "abstract": "Various techniques have been proposed for efficient evaluation of XPath expressions, where the XPath location steps are rooted in a single sequence of context nodes. Among these techniques, the staircase join allows to evaluate XPath location steps along arbitrary axes in at most one scan over the XML document, exploiting the XPath accelerator encoding (aka. pre/post encoding).In XQuery, however, embedded XPath sub-expressions occur in arbitrarily nested for-loops. Thus, they are rooted in multiple sequences of context nodes (one per iteration). Consequently, the previously proposed algorithms need to be applied repeatedly, requiring multiple scans over the XML document encoding.In this work, we present loop-lifted staircase join, an extension of the staircase join that allows to efficiently evaluate XPath sub-expressions in arbitrarily nested XQuery iteration scopes with only a single scan over the document. We implemented the loop-lifted staircase join in MonetDB/XQuery, that uses the XQuery-to-Relational Algebra compiler Pathfinder on top of the extensible RDBMS MonetDB. Performance results indicate that the proposed technique allows to build a system that is capable of efficiently evaluating XQuery queries including embedded XPath expressions, obtaining interactive query execution times for all XMark queries even on multi-gigabyte XML documents"}}, {"bib": {"title": "Defining the xml schema matching problem for a personal schema based query answering system", "pub_year": 2004, "author": "Marko Smiljanic and Maurice van Keulen and Willem Jonker", "number": "CT-TR-2004-1", "pages": "1-30", "publisher": "University of Twente, Centre for Telematics and"}}, {"bib": {"title": "Bridging the GAP between relational and native XML storage with staircase join", "pub_year": 2003, "author": "Jens Teubner and Maurice van Keulen and Torsten Grust", "conference": "15. GI Workshop on Foundations of Database Systems", "abstract": "Several mapping schemes have recently been proposed to store XML data in relational tables. Relational database systems are readily available and can handle vast amounts of data very efficiently, taking advantage of physical properties that are specific to the relational model, like sortedness or uniqueness. Tables that originate from XML documents, however, carry some further properties that cannot be exploited by current relational query processors. We propose a new join algorithm that is specifically designed to operate on XML data mapped to relational tables. The staircase join is fully aware of the underlying tree properties and allows for I/O and cache optimal query execution. As a local change to the database kernel, it can easily be plugged into any relational database and allows for various optimization strategies, e. g. selection pushdown. Experiments with our prototype, based on the Monet database kernel, have confirmed these statements."}}, {"bib": {"title": "Predicting parking occupancy via machine learning in the web of things", "pub_year": 2020, "author": "Jesper C Provoost and Andreas Kamilaris and Luc JJ Wismans and Sander J van der Drift and Maurice van Keulen", "journal": "Internet of Things", "volume": "12", "pages": "100301", "publisher": "Elsevier", "abstract": "The Web of Things (WoT) enables information gathered by sensors deployed in urban environments to be easily shared utilizing open Web standards and semantic technologies, creating easier integration with other Web-based information, towards advanced knowledge. Besides WoT, an essential aspect of understanding dynamic urban systems is artificial intelligence (AI). Via AI, data produced by WoT-enabled sensory observations can be analyzed and transformed into meaningful information, which describes and predicts current and future situations in time and space. This paper examines the impact of WoT and AI in smart cities, considering a real-world problem, the one of predicting parking availability. Traffic cameras are used as WoT sensors, together with weather forecasting Web services. Machine learning (ML) is employed for AI analysis, using predictive models based on neural networks and random \u2026"}}, {"bib": {"title": "A benchmark for online non-blocking schema transformations", "pub_year": 2015, "author": "Lesley Wevers and Matthijs Hofstra and Menno Tammens and Marieke Huisman and Maurice van Keulen", "publisher": "SciTePress", "abstract": "This paper presents a benchmark for measuring the blocking behavior of schema transformations in relational database systems. As a basis for our benchmark, we have developed criteria for the functionality and performance of schema transformation mechanisms based on the characteristics of state of the art approaches. To address limitations of existing approaches, we assert that schema transformations must be composable while satisfying the ACID guarantees like regular database transactions. Additionally, we have identified important classes of basic and complex relational schema transformations that a schema transformation mechanism should be able to perform. Based on these transformations and our criteria, we have developed a benchmark that extends the standard TPC-C benchmark with schema transformations, which can be used to analyze the blocking behavior of schema transformations in database systems. The goal of the benchmark is not only to evaluate existing solutions for non-blocking schema transformations, but also to challenge the database community to find solutions that allow more complex transactional schema transformations."}}, {"bib": {"title": "Hadoop for EEG Storage and Processing: A Feasibility Study", "pub_year": 2014, "author": "Ghita Berrada and Maurice van Keulen and Mena B Habib", "conference": "International Conference on Brain Informatics and Health", "pages": "218-230", "publisher": "Springer International Publishing", "abstract": "Lots of heterogeneous complex data are collected for diagnosis purposes. Such data should be shared between all caregivers and, often at least partly automatically processed, due to its complexity, for its full potential to be harnessed. This paper is a feasibility study that assesses the potential of Hadoop as a medical data storage and processing platform using EEGs as example of medical data."}}, {"bib": {"title": "Sample-based XPath Ranking for Web Information Extraction", "pub_year": 2013, "author": "Oliver Jundt and Maurice Van Keulen", "publisher": "Atlantis Press", "abstract": "Web information extraction typically relies on a wrapper, ie, program code or a configuration that specifies how to extract some information from web pages at a specific website. Manually creating and maintaining wrappers is a cumbersome and error-prone task. It may even be prohibitive as some applications require information extraction from previously unseen websites. This paper approaches the problem of automatic on-the-fly wrapper creation for websites that provide attribute data for objects in a \u2018search\u2013search result page\u2013detail page\u2019setup. The approach is a wrapper induction approach which uses a small and easily obtainable set of sample data for ranking XPaths on their suitability for extracting the wanted attribute data. Experiments show that the automatically generated top-ranked XPaths indeed extract the wanted data. Moreover, it appears that 20 to 25 input samples suffice for finding a suitable XPath for an attribute."}}, {"bib": {"title": "Named Entity Extraction and Disambiguation: The Missing Link", "pub_year": 2013, "author": "Mena B Habib and Maurice van Keulen", "publisher": "ACM"}}, {"bib": {"title": "Improving toponym extraction and disambiguation using feedback loop", "pub_year": 2012, "author": "Mena B Habib and Maurice van Keulen", "conference": "International Conference on Web Engineering", "pages": "439-443", "publisher": "Springer Berlin Heidelberg", "abstract": "This paper addresses two problems with toponym extraction and disambiguation. First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disambiguation techniques mostly take as input extracted toponyms without considering the uncertainty and imperfection of the extraction process.It is the aim of this paper to investigate both avenues and to show that explicit handling of the uncertainty of annotation has much potential for making both extraction and disambiguation more robust."}}, {"bib": {"title": "Compression of probabilistic XML documents", "pub_year": 2009, "author": "Irma Veldman and Ander De Keijzer and Maurice Van Keulen", "conference": "International Conference on Scalable Uncertainty Management", "pages": "255-267", "publisher": "Springer Berlin Heidelberg", "abstract": "Database techniques to store, query and manipulate data that contains uncertainty receives increasing research interest. Such UDBMSs can be classified according to their underlying data model: relational, XML, or RDF. We focus on uncertain XML DBMS with as representative example the Probabilistic XML model (PXML) of [10,9]. The size of a PXML document is obviously a factor in performance. There are PXML-specific techniques to reduce the size, such as a push down mechanism, that produces equivalent but more compact PXML documents. It can only be applied, however, where possibilities are dependent. For normal XML documents there also exist several techniques for compressing a document. Since Probabilistic XML is (a special form of) normal XML, it might benefit from these methods even more. In this paper, we show that existing compression mechanisms can be combined with PXML \u2026"}}, {"bib": {"title": "User feedback in probabilistic xml", "pub_year": 2007, "author": "Ander de Keijzer and Maurice van Keulen", "number": "TR-CTI", "publisher": "Centre for Telematics and Information Technology, University of Twente", "abstract": "Data integration is a challenging problem in many application areas. Approaches mostly attempt to resolve semantic uncertainty and conflicts between information sources as part of the data integration process. In some application areas, this is impractical or even prohibitive, for example, in an ambient environment where devices on an ad hoc basis have to exchange information autonomously. We have proposed a probabilistic XML approach that allows data integration without user involvement by storing semantic uncertainty and conflicts in the integrated XML data. As a consequence, the integrated information source represents all possible appearances of objects in the real world, the so-called possible worlds.In this paper, we show how user feedback on query results can resolve semantic uncertainty and conflicts in the integrated data. Hence, user involvement is effectively postponed to query time, when a user is already interacting actively with the system. The technique relates positive and negative statements on query answers to the possible worlds of the information source thereby either reinforcing, penalizing, or eliminating possible worlds. We show that after repeated user feedback, an integrated information source better resembles the real world and may converge towards a non-probabilistic information source."}}, {"bib": {"title": "Supporting positional predicates in efficient XPath axis evaluation for DOM data structures", "pub_year": 2004, "author": "Torsten Grust and Jan Hidders and Philippe Michiels and Roel Vercammen and Maurice Van Keulen", "publisher": "Technical Report TR2004-05, University of Antwerp and University of Twente and University of Konstanz", "abstract": "In this technical report we propose algorithms for implementing the axes for element nodes in XPath given a DOM-like representation of the document. First, we construct algorithms for evaluating simple step expressions, withoout any (positional) predicates. The time complexity of these algorithms is at most O (l+ m) where l is the size of the input list and m the size of the output list. This improves upon results in [6] where also algorithms with linear time complexity are presented, but these are linear in the size of the entire document whereas our algorithms are linear in the size of the intermediate results which are often much smaller.In a second phase we give a description of how the support for positional predicates can be added to the algorithms with a focus on maintaining the efficiency of evaluation. Each algorithm assumes an input list that is sorted in document order and duplicate-free and returns a sorted and duplicate-free list of the result of following a certain axis from the nodes in the input list."}}, {"bib": {"title": "Increasing NER recall with minimal precision loss", "pub_year": 2013, "author": "Jasper Kuperus and Cor J Veenman and Maurice van Keulen", "conference": "2013 European Intelligence and Security Informatics Conference", "pages": "106-111", "publisher": "IEEE", "abstract": "Named Entity Recognition (NER) is broadly used as a first step toward the interpretation of text documents. However, for many applications, such as forensic investigation, recall is currently inadequate, leading to loss of potentially important information. Entity class ambiguity cannot be resolved reliably due to the lack of context information or the exploitation thereof. Consequently, entity classification introduces too many errors, leading to severe omissions in answers to forensic queries. We propose a technique based on multiple candidate labels, effectively postponing decisions for entity classification to query time. Entity resolution exploits user feedback: a user is only asked for feedback on entities relevant to his/her query. Moreover, giving feedback can be stopped anytime when query results are considered good enough. We propose several interaction strategies that obtain increased recall with little loss in \u2026"}}, {"bib": {"title": "Computer assisted extraction, merging and correlation of identities with tracks inspector", "pub_year": 2013, "author": "Jop Hofste and Hans Henseler and Maurice van Keulen", "conference": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law", "pages": "247-248", "publisher": "ACM", "abstract": "With the pervasiveness of computers and mobile devices, digital forensics becomes more important in law enforcement. Detectives increasingly depend on the scarce support of digital specialists which impedes efficiency of criminal investigations. Tracks Inspector is a commercial solution that enables non-technical investigators to easily investigate digital evidence using a web browser. We will demonstrate how Tracks Inspector can be used to discover the most important persons and groups in case data by investigators without requiring the help of digital forensics experts."}}, {"bib": {"title": "Indeterministic handling of uncertain decisions in duplicate detection", "pub_year": 2010, "author": "Fabian Panse and Maurice van Keulen and Norbert Ritter", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "In current research, duplicate detection is usually considered as a deterministic approach in which tuples are either declared as duplicates or not. However, most often it is not completely clear whether two tuples represent the same real-world entity or not. In deterministic approaches, however, this uncertainty is ignored, which in turn can lead to false decisions. In this paper, we present an indeterministic approach for handling uncertain decisions in a duplicate detection process by using a probabilistic target schema. Thus, instead of deciding between multiple possible worlds, all these worlds can be modeled in the resulting data. This approach minimizes the negative impacts of false decisions. Furthermore, the duplicate detection process becomes almost fully automatic and human effort can be reduced to a large extent. Unfortunately, a full-indeterministic approach is by definition too expensive (in time as well as in storage) and hence impractical. For that reason, we additionally introduce several semi-indeterministic methods for heuristically reducing the set of indeterministically handled decisions in a meaningful way."}}, {"bib": {"title": "Run-time Optimization for Pipelined Systems", "pub_year": 2010, "author": "R Abdel Kader and Maurice Van Keulen and Peter Boncz and Stefan Manegold", "publisher": "CEUR-WS. org", "abstract": "Traditional optimizers fail to pick good execution plans, when faced with increasingly complex queries and large data sets. This failure is even more acute in the context of XQuery, due to the structured nature of the XML language. To overcome the vulnerabilities of traditional optimizers, we have previously proposed ROX, a Run-time Optimizer for XQueries, which interleaves optimization and execution of full tables. ROX has proved to be robust, even in the presence of strong correlations, but it has one limitation: it uses full materialization of intermediate results making it unsuitable for pipelined systems. Therefore, this paper proposes ROX-sampled, a variant of ROX, which executes small data samples, thus generating smaller intermediates. We conduct extensive experiments which proved that ROX-sampled is comparable to ROX in performance, and that it is still robust against correlations. The main benefit of ROX-sampled is that it allows the large number of pipelined databases to import the ROX idea into their optimization paradigm."}}, {"bib": {"title": "Relational Approach to Logical Query Optimization of XPath.", "pub_year": 2004, "author": "Maurice van Keulen", "conference": "TDM", "pages": "57-63", "abstract": "To be able to handle the ever growing volumes of XML documents, effective and efficient data management solutions are needed. Managing XML data in a relational DBMS has great potential. Recently, effective relational storage schemes and index structures have been proposed as well as special-purpose join operators to speed up querying of XML data using XPath/XQuery. In this paper, we address the topic of query plan construction and logical query optimization. The claim of this paper is that standard relational algebra extended with special-purpose join operators suffices for logical query optimization. We focus on the XPath accelerator storage scheme and associated staircase join operators, but the approach can be generalized easily."}}, {"bib": {"title": "A probabilistic database extension", "pub_year": 2004, "author": "Ander Keijzer and Maurice Keulen", "number": "TR-CTIT-04-21", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)", "abstract": "Data exchange between embedded systems and other small or large computing devices increases. Since data in different data sources may refer to the same real world objects, data cannot simply be merged. Furthermore, in many situations, conflicts in data about the same real world objects need to be resolved without interference from a user. In this report, we report on an attempt to make a RDBMS probabilistic, ie, data in a relation represents all possible views on the real world, in order to achieve unattended data integration. We define a probabilistic relational data model and review standard SQL query primitives in the light of probabilistic data. It appears that thinking in terms of \u2018possible worlds\u2019 is powerful in determining the proper semantics of these query primitives."}}, {"bib": {"title": "ISIS in the Eyes of the Dutch", "pub_year": 2017, "author": "Bas Hendrikse and Mena B Habib and Maurice van Keulen", "publisher": "Springer Verlag", "abstract": "The presence of militant group Islamic State of Iraq and Syria (ISIS) is growing. Terrorist attacks in Europe and an incoming stream of refugees in the south of the continent are some of the reasons Europe is getting socially involved in the Middle-Eastern war. It might seem that the Netherlands could become a target of the organization too. But are Dutch citizens concerned about this? In this paper, we describe the reaction of the Dutch on ISIS by analyzing what they say on Twitter about the organization. With the use of text classification, topic modeling and visualization tools, we were able to retrieve Tweets about ISIS and create a network graph displaying the ten main topics about ISIS which Dutch people tweeted about, in addition to a word cloud, displaying the words which were most used in the Tweets. The visualizations are used to analyze what topics people discussed most regarding ISIS on Twitter and how they felt about these topics. Understanding the social network responses to ISIS\u2019attacks can give us some clues to understand its impact on the society."}}, {"bib": {"title": "Improving named entity disambiguation by iteratively enhancing certainty of extraction", "pub_year": 2011, "author": "Mena B Habib and Maurice van Keulen", "journal": "Faculty of EEMCS, University of Twente, The Netherlands", "abstract": "Named entity extraction and disambiguation have received much attention in recent years. Typical fields addressing these topics are information retrieval, natural language processing, and semantic web. This paper addresses two problems with named entity extraction and disambiguation. First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disambiguation techniques mostly take as input extracted named entities without considering the uncertainty and imperfection of the extraction process.It is the aim of this paper to investigate both avenues and to show that explicit handling of the uncertainty of annotation has much potential for making both extraction and disambiguation more robust. We conducted experiments with a set of holiday home descriptions with the aim to extract and disambiguate toponyms as a representative example of named entities. We show that the effectiveness of extraction influences the effectiveness of disambiguation, and reciprocally, how retraining the extraction models with information automatically derived from the disambiguation results, improves the extraction models. This mutual reinforcement is shown to even have an effect after several iterations."}}, {"bib": {"title": "Onzekere databases", "pub_year": 2010, "author": "Maurice van Keulen", "journal": "DB/M: database magazine", "volume": "21", "number": "4", "pages": "22-27", "publisher": "Samson", "abstract": "Maurice van Keulen\u2018fact of life\u2019. De uitdaging is om informatiesystemen soepeler met onvolkomenheden om te kunnen laten gaan. Dat is niet alleen een kwestie van controles en data cleaning, maar ook van robuustheid en flexibiliteit. Alle bovengenoemde soorten onvolkomenheden zijn te zien als onzekerheid: we weten simpelweg op een bepaald moment niet hoe het werkelijk zit. Door eerlijk en expliciet met die onzekerheid om te gaan, kan een informatiesysteem zich daaraan aanpassen en goed genoeg functioneren ondanks dat niet alle gegevens precies en correct beschikbaar zijn.Onzekere databases maken het mogelijk om eerlijk en expliciet met onzekerheid om te gaan, zonder dat het de complexiteit van de informatiesystemen significant verhoogt. Kort gezegd, een onzekere database maakt het mogelijk om niet alleen de data maar ook de onzekerheid over die data expliciet op te slaan. Vervolgens zijn \u2026"}}, {"bib": {"title": "Qualitative effects of knowledge rules in probabilistic data integration", "pub_year": 2008, "author": "Maurice van Keulen and Ander de Keijzer", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "One of the problems in data integration is data overlap: the fact that different data sources have data on the same real world entities. Much development time in data integration projects is devoted to entity resolution. Often advanced similarity measurement techniques are used to remove semantic duplicates from the integration result or solve other semantic conflicts, but it proofs impossible to get rid of all semantic problems in data integration. An often-used rule of thumb states that about 90% of the development effort is devoted to solving the remaining 10% hard cases. In an attempt to significantly decrease human effort at data integration time, we have proposed an approach that stores any remaining semantic uncertainty and conflicts in a probabilistic database enabling it to already be meaningfully used. The main development effort in our approach is devoted to defining and tuning knowledge rules and thresholds. Rules and thresholds directly impact the size and quality of the integration result. We measure integration quality indirectly by measuring the quality of answers to queries on the integrated data set in an information retrieval-like way. The main contribution of this report is an experimental investigation of the effects and sensitivity of rule definition and threshold tuning on the integration quality. This proves that our approach indeed reduces development effort\u2014and not merely shifts the effort to rule definition and threshold tuning\u2014by showing that setting rough safe thresholds and defining only a few rules suffices to produce a \u2018good enough\u2019integration that can be meaningfully used."}}, {"bib": {"title": "Report on the first VLDB workshop on Management of Uncertain Data (MUD)", "pub_year": 2007, "author": "Ander de Keijzer and Maurice van Keulen and Alex Dekhtyar", "journal": "ACM SIGMOD Record", "volume": "36", "number": "4", "pages": "57-58", "publisher": "ACM", "abstract": "On Monday September 24th, we organized the first international VLDB workshop on Management of Uncertain Data [dKvKD07]. The idea of this workshop arose a year earlier at the Twente Data Management Workshop on Uncertainty in Databases [dKvK06]. The TDM is a bi-annual workshop organized by the Database group of the University of Twente, for which each time a different topic is chosen. The participants of TDM 2006 were enthusiastic about the topic \"Uncertainty in Databases\" and strongly expressed the wish for a follow-up co-located with an international conference. To fulfill this wish, we organized the MUD-workshop at VLDB."}}, {"bib": {"title": "Information Integration-the process of integration, evolution and versioning", "pub_year": 2005, "author": "Ander Keijzer and Maurice Keulen", "number": "CTIT-TR-05-58", "publisher": "University of Twente, Centre for Telematica and Information Technology (CTIT)", "abstract": "At present, many information sources are available wherever you are. Most of the time, the information needed is spread across several of those information sources. Gathering this information is a tedious and time consuming job. Automating this process would assist the user in its task. Integration of the information sources provides a global information source with all information needed present. All of these information sources also change over time. With each change of the information source, the schema of this source can be changed as well. The data contained in the information source, however, cannot be changed every time, due to the huge amount of data that would have to be converted in order to conform to the most recent schema. In this report we describe the current methods to information integration, evolution and versioning. We distinguish between integration of schemas and integration of the actual data. We also show some key issues when integrating XML data sources."}}, {"bib": {"title": "Two video analysis applications using foreground/background segmentation", "pub_year": 2003, "author": "Zoran Zivkovic and Milan Petkovic and R Van Mierlo and M van Keulen and F van der Heijden and W Jonker and E Rijnierse", "conference": "Visual Information Engineering, 2003. VIE 2003. International Conference on", "pages": "310-313", "publisher": "IET", "abstract": "Probably the most frequently solved problem when videos are analyzed is segmenting a foreground object from its background in an image. After some regions in an image are detected as the foreground objects, some features are extracted that describe the segmented regions. These features together with the domain knowledge are often enough to extract the needed high-level semantics from the video material. In this paper we present two automatic systems for video analysis and indexing. In both systems the segmentation of the foreground objects is the basic processing step. The extracted features are then used to solve the problem. One system analyses traffic videos, the other tennis games."}}, {"bib": {"title": "An architecture and methodology for the design and development of Technical Information Systems", "pub_year": 1996, "author": "R Capobianchi and M Mautref and Maurice van Keulen and Herman Balsters", "conference": "International Symposium on Methodologies for Intelligent Systems", "pages": "511-520", "publisher": "Springer Berlin Heidelberg", "abstract": "In order to meet demands in the context of Technical Information Systems (TIS) pertaining to reliability, extensibility, maintainability, etc., we have developed an architectural framework with accompanying methodological guidelines for designing such systems. With the framework, we aim at complex multiapplication information systems using a repository to share data among applications. The framework proposes to keep a strict separation between Man-Machine-Interface and Model data, and provides design and implementation support to do this effectively.The framework and methodological guidelines have been developed in the context of the ESPRIT project IMPRESS. The project also provided for \u201ctesting grounds\u201d in the form of a TIS for the Spanish Electricity company Iberdrola."}}, {"bib": {"title": "Automatic Process Comparison for Subpopulations: Application in Cancer Care", "pub_year": 2020, "author": "Francesca Marazza and Faiza Allah Bukhsh and Jeroen Geerdink and Onno Vijlbrief and Shreyasi Pathak and Maurice van Keulen and Christin Seifert", "journal": "International journal of environmental research and public health", "volume": "17", "number": "16", "pages": "5707", "publisher": "Multidisciplinary Digital Publishing Institute", "abstract": "Processes in organisations, such as hospitals, may deviate from the intended standard processes, due to unforeseeable events and the complexity of the organisation. For hospitals, the knowledge of actual patient streams for patient populations (eg, severe or non-severe cases) is important for quality control and improvement. Process discovery from event data in electronic health records can shed light on the patient flows, but their comparison for different populations is cumbersome and time-consuming. In this paper, we present an approach for the automatic comparison of process models that were extracted from events in electronic health records. Concretely, we propose comparing processes for different patient populations by cross-log conformance checking, and standard graph similarity measures obtained from the directed graph underlying the process model. We perform a user study with 20 participants in order to obtain a ground truth for similarity of process models. We evaluate our approach on two data sets, the publicly available MIMIC database with the focus on different cancer patients in intensive care, and a database on breast cancer patients from a Dutch hospital. In our experiments, we found average fitness to be a good indicator for visual similarity in the ZGT use case, while the average precision and graph edit distance are strongly correlated with visual impression for cancer process models on MIMIC. These results are a call for further research and evaluation for determining which similarity or combination of similarities is needed in which type of process model comparison. View Full-Text"}}, {"bib": {"title": "Short Term Prediction of Parking Area states Using Real Time Data and Machine Learning Techniques", "pub_year": 2019, "author": "Jesper Provoost and Luc Wismans and Sander Van der Drift and Andreas Kamilaris and Maurice Van Keulen", "journal": "arXiv preprint arXiv:1911.13178", "abstract": "Public road authorities and private mobility service providers need information derived from the current and predicted traffic states to act upon the daily urban system and its spatial and temporal dynamics. In this research, a real-time parking area state (occupancy, in- and outflux) prediction model (up to 60 minutes ahead) has been developed using publicly available historic and real time data sources. Based on a case study in a real-life scenario in the city of Arnhem, a Neural Network-based approach outperforms a Random Forest-based one on all assessed performance measures, although the differences are small. Both are outperforming a naive seasonal random walk model. Although the performance degrades with increasing prediction horizon, the model shows a performance gain of over 150% at a prediction horizon of 60 minutes compared with the naive model. Furthermore, it is shown that predicting the in- and outflux is a far more difficult task (i.e. performance gains of 30%) which needs more training data, not based exclusively on occupancy rate. However, the performance of predicting in- and outflux is less sensitive to the prediction horizon. In addition, it is shown that real-time information of current occupancy rate is the independent variable with the highest contribution to the performance, although time, traffic flow and weather variables also deliver a significant contribution. During real-time deployment, the model performs three times better than the naive model on average. As a result, it can provide valuable information for proactive traffic management as well as mobility service providers."}}, {"bib": {"title": "Rule-Based Conditioning of Probabilistic Data", "pub_year": 2018, "author": "Maurice van Keulen and Benjamin L Kaminski and Christoph Matheja and Joost-Pieter Katoen", "conference": "International Conference on Scalable Uncertainty Management", "pages": "290-305", "publisher": "Springer, Cham", "abstract": "Data interoperability is a major issue in data management for data science and big data analytics. Probabilistic data integration (PDI) is a specific kind of data integration where extraction and integration problems such as inconsistency and uncertainty are handled by means of a probabilistic data representation. This allows a data integration process with two phases: (1) a quick partial integration where data quality problems are represented as uncertainty in the resulting integrated data, and (2) using the uncertain data and continuously improving its quality as more evidence is gathered. The main contribution of this paper is an iterative approach for incorporating evidence of users in the probabilistically integrated data. Evidence can be specified as hard or soft rules (i.e., rules that are uncertain themselves)."}}, {"bib": {"title": "Truth assessment of objective facts extracted from tweets: A case study on world cup 2014 game facts", "pub_year": 2017, "author": "Bas Janssen and Mena Habib and Maurice Van Keulen", "conference": "13th International Conference on Web Information Systems and Technologies, WEBIST 2017", "pages": "187-195", "publisher": "SCITEPRESS", "abstract": "By understanding the tremendous opportunities to work with social media data and the acknowledgment of the negative effects social media messages can have, a way of assessing truth in claims on social media would not only be interesting but also very valuable. By making use of this ability, applications using social media data could be supported, or a selection tool in research regarding the spread of false rumors or\u2019fake news\u2019 could be build. In this paper, we show that we can determine truth by using a statistical classifier supported by an architecture of three preprocessing phases. We base our research on a dataset of Twitter messages about the FIFA World Cup 2014. We determine the truth of a tweet by using 7 popular fact types (involving events in the matches in the tournament such as scoring a goal) and we show that we can achieve an F1-score of 0.988 for the first class; the Tweets which contain no false facts and an F1-score of 0.818 on the second class; the Tweets which contain one or more false facts."}}, {"bib": {"title": "Efficient web harvesting strategies for monitoring deep web content", "pub_year": 2016, "author": "Mohammad Khelghati and Djoerd Hiemstra and Maurice van Keulen", "conference": "Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services", "pages": "389-393", "publisher": "ACM", "abstract": "Web content changes rapidly [18]. In Focused Web Harvesting [17] which aim it is to achieve a complete harvest for a given topic, this dynamic nature of the web creates problems for users who need to access a set of all the relevant web data to their topics of interest. Whether you are a fan following your favorite idol or a journalist investigating a topic, you may need not only to access all the relevant information but also the recent changes and updates. General search engines like Google apply several techniques to enhance the freshness of their crawled data. However, in focused web harvesting, we lack an efficient approach that detects changes for a given topic over time. In this paper, we focus on techniques that can keep the relevant content to a given query up-to-date. To do so, we test four different approaches to efficiently harvest all the changed documents matching a given entity by querying web search \u2026"}}, {"bib": {"title": "Lazy Evaluation for Concurrent OLTP and Bulk Transactions", "pub_year": 2016, "author": "Lesley Wevers and Marieke Huisman and Maurice van Keulen", "conference": "Proceedings of the 20th International Database Engineering & Applications Symposium", "pages": "115-124", "publisher": "ACM", "abstract": "Existing concurrency control systems cannot execute transactions with overlapping updates concurrently. This is especially problematic for bulk updates, which usually overlap with all concurrent transactions. To solve this, we have developed a concurrency control mechanism based on lazy evaluation, which moves evaluation of operations from the writer to the reader. This allows readers to prioritize evaluation of those operations in which they are interested, without loss of atomicity of transactions. To handle bulk operations, we dynamically split large transactions into transactions on smaller parts of the data. In this paper we present an abstract lazy index structure for lazy transactions, and show how transactions can be encoded to effectively use this data structure. Moreover, we discuss evaluation strategies for lazy transactions, where trade-offs can be made between latency and throughput. To evaluate our \u2026"}}, {"bib": {"title": "Towards complete coverage in focused web harvesting", "pub_year": 2015, "author": "Mohammadreza Khelghati and Djoerd Hiemstra and Maurice van Keulen", "pages": "1-9", "abstract": "With the goal of harvesting all information about a given entity, in this paper, we try to harvest all matching documents for a given query submitted on a search engine. The objective is to retrieve all information about for instance\" Michael Jackson\",\" Islamic State\", or\" FC Barcelona\" from indexed data in search engines, or hidden data behind web forms, using a minimum number of queries. Policies of web search engines usually do not allow accessing all of the matching query search results for a given query. They limit the number of returned documents and the number of user requests. These limitations are also applied in deep web sources, for instance in social networks like Twitter. In this work, we propose a new approach which automatically collects information related to a given query from a search engine, given the search engine's limitations. The approach minimizes the number of queries that need to be sent \u2026"}}, {"bib": {"title": "Handling uncertainty in relation extraction: a case study on tennis tournament results extraction from tweets", "pub_year": 2015, "author": "Jochem GJ Verburg and Mena B Habib and Maurice van Keulen", "pages": "1-4", "abstract": "Relation extraction involves different types of uncertainty due to the imperfection of the extraction tools and the inherent ambiguity of unstructured text. In this paper, we discuss several ways of handling uncertainties in relation extraction from social media. Our study case is to extract tennis games' results for two Grand Slam tennis tournaments from tweets. Analysis has been done to find to what extent it is useful to use semantic web, domain knowledge, facts repetition, and authors' trustworthiness to improve the certainty of the extracted relations."}}, {"bib": {"title": "Analysis of the blocking behaviour of schema transformations in relational database systems", "pub_year": 2015, "author": "Lesley Wevers and Matthijs Hofstra and Menno Tammens and Marieke Huisman and Maurice van Keulen", "conference": "East European Conference on Advances in Databases and Information Systems", "pages": "169-183", "publisher": "Springer International Publishing", "abstract": "In earlier work we have extended the TPC-C benchmark with basic and complex schema transformations. This paper uses this benchmark to investigate the blocking behaviour of online schema transformations in PostgreSQL, MySQL and Oracle 11g. First we discuss experiments using the data definition language of the DBMSs, which show that all complex operations are blocking, while we have mixed results for basic transformations. Second, we look at a technique for online schema transformations by Ronstr\u00f6m, based on triggers. Our experiments show that pt-online-schema-change for MySQL and DBMS_REDEFINITION for Oracle can perform basic transformations without blocking, however, support for complex transformations is missing. To conclude, we provide a solution outline for complex non-blocking transformations."}}, {"bib": {"title": "Spatiotemporal Behavior Profiling: A Treasure Hunt Case Study", "pub_year": 2015, "author": "Victor de Graaff and Dieter Pfoser and Maurice van Keulen and Rolf A De By", "publisher": "Springer Verlag", "abstract": "Trajectories have been providing us with a wealth of derived information such as traffic conditions and road network updates. This work focuses on deriving user profiles through spatiotemporal analysis of trajectory data to provide insight into the quality of information provided by users. The presented behavior profiling method assesses user participation characteristics in a treasure-hunt type event. Consisting of an analysis and a profiling phase, analysis involves a timeline and a stay-point analysis, as well as a semantic trajectory inspection relating actual and expected paths. The analysis results are then grouped around profiles that can be used to estimate the user performance in the activity. The proposed profiling method is evaluated by means of a student orientation treasure-hunt activity at the University of Twente, The Netherlands. The profiling method is used to predict the students\u2019 gaming behavior \u2026"}}, {"bib": {"title": "Towards Online and Transactional Relational Schema Transformations", "pub_year": 2014, "author": "Lesley Wevers and Matthijs Hofstra and Menno Tammens and Marieke Huisman and Maurice Keulen", "number": "TR-CTIT-14-10", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)", "abstract": "In this paper, we want to draw the attention of the database community to the problem of online schema changes: changing the schema of a database without blocking concurrent transactions. We have identified important classes of relational schema transformations that we want to perform online, and we have identified general requirements for the mechanisms that execute these transformations. Using these requirements, we have developed an experiment based on the standard TPC-C benchmark to assess the behaviour of existing systems. We look at PostgreSQL, which does not support online schema changes; MySQL, which supports basic online schema changes; and pt-online-schema-change, which is a tool for MySQL that uses triggers to implement online schema changes. We found that none of the existing systems fulfill our requirements. In particular, existing non-blocking solutions can not maintain the ACID guarantees when composing schema transformations. This leads to intermediate states being exposed to database programs, which are nontrivial to handle correctly. As a solution direction, we propose lazy schema transformations, which can naturally be composed into complex schema transformations that properly guarantee the ACID properties, and which have minimal impact on concurrent transactions."}}, {"bib": {"title": "Designing A General Deep Web Access Approach Based On A Newly Introduced Factor; Harvestability Factor (HF)", "pub_year": 2014, "author": "Mohammadreza Khelghati and Maurice Keulen and Djoerd Hiemstra", "number": "TR-CTIT-14-08", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)", "abstract": "The growing need of accessing more and more information draws attentions to huge amount of data hidden behind web forms defined as deep web. To make this data accessible, harvesters have a crucial role. Targeting different domains and websites enhances the need to have a general-purpose harvester which can be applied to different settings and situations. To develop such a harvester, a number of issues should be considered. Among these issues, business domain features, targeted websites' features, and the harvesting goals are the most influential ones. To consider all these elements in one big picture, a new concept, called harvestability factor (HF), is introduced in this paper. The HF is defined as an attribute of a website (HFW) or a harvester (HFH) representing the extent to which the website can be harvested or the harvester can harvest. The comprising elements of these factors are different websites'(for HFW) or harvesters'(for HFH) features. These features are presented in this paper by gathering a number of them from literature and introducing new ones through the authors' experiments. In addition to enabling websites' or harvesters' designers of evaluating where they products stand from the harvesting perspective, the HF can act as a framework for designing general purpose deep web harvesters. This framework allows filling in the gap in designing general purpose harvesters by focusing on detailed features of deep websites which have effects on harvesting processes. The represented features in this paper provide a thorough list of requirements for designing deep web harvesters which is not done to best of our knowledge \u2026"}}, {"bib": {"title": "Uncertainty Handling in Named Entity Extraction and Disambiguation for Informal Text", "pub_year": 2014, "author": "Maurice van Keulen and Mena B Habib", "pages": "309-328", "publisher": "Springer International Publishing", "abstract": "Social media content represents a large portion of all textual content appearing on the Internet. These streams of user generated content (UGC) provide an opportunity and challenge for media analysts to analyze huge amount of new data and use them to infer and reason with new information. A main challenge of natural language is its ambiguity and vagueness. To automatically resolve ambiguity, the grammatical structure of sentences is used. However, when we move to informal language widely used in social media, the language becomes more ambiguous and thus more challenging for automatic understanding.Information Extraction (IE) is the research field that enables the use of unstructured text in a structured way. Named Entity Extraction (NEE) is a sub task of IE that aims to locate phrases (mentions) in the text that represent names of entities such as persons, organizations or locations \u2026"}}, {"bib": {"title": "Toponym Extraction and Disambiguation Enhancement Using Loops of Feedback", "pub_year": 2013, "author": "Mena B Habib and Maurice van Keulen", "pages": "113-129", "publisher": "Springer Berlin Heidelberg", "abstract": "Toponym extraction and disambiguation have received much attention in recent years. Typical fields addressing these topics are information retrieval, natural language processing, and semantic web. This paper addresses two problems with toponym extraction and disambiguation. First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disambiguation techniques mostly take as input extracted named entities without considering the uncertainty and imperfection of the extraction process. In this paper we aim to investigate both avenues and to show that explicit handling of the uncertainty of annotation has much potential for making both extraction and disambiguation more robust. We conducted experiments with a set of holiday home descriptions with the aim to extract and disambiguate toponyms. We show that the extraction confidence probabilities are \u2026"}}, {"bib": {"title": "Digital-forensics based pattern recognition for discovering identities in electronic evidence", "pub_year": 2013, "author": "Hans Henseler and Jop Hofst\u00e9 and Maurice van Keulen", "publisher": "IEEE Computer Society", "abstract": "With the pervasiveness of computers and mobile devices, digital forensics becomes more important in law enforcement. Detectives increasingly depend on the scarce support of digital specialists which impedes efficiency of criminal investigations. This paper proposes and algorithm to extract, merge and rank identities that are encountered in the electronic evidence during processing. Two experiments are described demonstrating that our approach can assist with the identification of frequently occurring identities so that investigators can prioritize the investigation of evidence units accordingly."}}, {"bib": {"title": "08421 Abstracts Collection--Uncertainty Management in Information Systems", "pub_year": 2009, "author": "Christoph Koch and Birgitta K\u00f6nig-Ries and Volker Markl and Maurice van Keulen", "journal": "Dagstuhl Seminar Proceedings", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik", "abstract": "From October 12 to 17, 2008 the Dagstuhl Seminar 08421'Uncertainty Management in Information Systems''was held in Schloss Dagstuhl~--~ Leibniz Center for Informatics. The abstracts of the plenary and session talks given during the seminar as well as those of the shown demos are put together in this paper."}}, {"bib": {"title": "Operating Systems", "pub_year": 2007, "author": "AL Schoute and M van Keulen", "number": "Chapte", "pages": "761-770", "publisher": "Reed Business Information", "abstract": "Operating Systems \u2014 University of Twente Research Information Skip to main navigation Skip \nto search Skip to main content University of Twente Research Information Logo Home Profiles \nResearch Units Research Output Datasets Activities Prizes Press / Media Search by expertise, \nname or affiliation Operating Systems Albert L. Schoute Databases (Former) Research output: \nChapter in Book/Report/Conference proceeding \u203a Chapter \u203a Academic Overview Original \nlanguage Undefined Title of host publication ICT-Zakboek Editors TMA Bemelmans, Maurice \nvan Keulen, RJ Kusters, M. Looijen Place of Publication Den Haag Publisher Reed Business \nInformation Pages 761-770 Number of pages 10 ISBN (Print) 978-90-6228-671-3 Publication \nstatus Published - Jul 2007 Publication series Name PBNA Polyzakboekjes Publisher \nReed Business Information Number Chapter VI.1.7 Keywords EWI-9507 IR-63970 METIS-://\u2026"}}, {"bib": {"title": "Effectiveness bounds for non-exhaustive schema matching systems", "pub_year": 2006, "author": "Marko Smiljanic and Maurice van Keulen and Willem Jonker", "conference": "22nd International Conference on Data Engineering Workshops (ICDEW'06)", "pages": "83-83", "publisher": "IEEE", "abstract": "Semantic validation of the effectiveness of a schema matching system is traditionally performed by comparing system-generated mappings with those of human evaluators. The human effort required for validation quickly becomes huge in large scale environments. The performance of a matching system, however, is not solely determined by the quality of the mappings, but also by the efficiency with which it can produce them. Improving efficiency quickly leads to a trade-off between efficiency and effectiveness. Establishing or obtaining a large test collection for measuring this trade-off is often a severe obstacle. In this paper, we present a technique for determining lower and upper bounds for effectiveness measures for a certain class of schema matching system improvements in order to lower the required validation effort. Effectiveness bounds for a matching system improvement are solely derived from a comparison \u2026"}}, {"bib": {"title": "The Dodo query flattening system", "pub_year": 2004, "author": "Joeri Ruth and Maarten Fokkinga and Maurice Keulen", "number": "TR-CTIT-04-41", "pages": "1-51", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)"}}, {"bib": {"title": "CIRQUID: complex information retrieval queries in a database", "pub_year": 2003, "author": "Djoerd Hiemstra and Arjen P Vries and Henk Ernst Blok and Maurice Keulen and Willem Jonker and Martin L Kersten", "number": "TR-CTIT-03-26", "pages": "1-18", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)", "abstract": "The CIRQUID project plans to design and build a DBMS that seemlessly integrates relevance-oriented querying of semi-structured data (XML) with traditional querying of this data. The project is funded by the Netherlands Organisation of Scientific Research (NWO project number 612.061. 210)."}}, {"bib": {"title": "A framework for representation, validation and implementation of database application semantics", "pub_year": 1995, "author": "Maurice van Keulen and Jacek Skowronek and Peter MG Apers and Herman Balsters and Henk M Blanken and RA de By and Jan Flokstra", "publisher": "University of Twente", "abstract": "New application domains in data-processing environments pose new requirements on the methodologies, techniques and tools used to design them. The applications\u2019 semantics should be fully represented at an increasingly high level, and the representation should be subject to rigorous validation and verification. We present a semantic representation framework (including the language, methods and tools) for design of data-processing applications. The new features of the framework include a small number of precisely defined domain-independent concepts, high-level possibilities for describing behavioural semantics (methods and constraints) and the validation and verification tools included in the framework. We present examples of the use of the framework, including the use of its tools."}}, {"bib": {"title": "Autoencoder-based cleaning in probabilistic databases", "pub_year": 2021, "author": "RR Mauritz and FPJ Nijweide and Jasper Goseling and Maurice van Keulen", "journal": "arXiv preprint arXiv:2106.09764", "abstract": "In the field of data integration, data quality problems are often encountered when extracting, combining, and merging data. The probabilistic data integration approach represents information about such problems as uncertainties in a probabilistic database. In this paper, we propose a data-cleaning autoencoder capable of near-automatic data quality improvement. It learns the structure and dependencies in the data to identify and correct doubtful values. A theoretical framework is provided, and experiments show that it can remove significant amounts of noise from categorical and numeric probabilistic data. Our method does not require clean data. We do, however, show that manually cleaning a small fraction of the data significantly improves performance."}}, {"bib": {"title": "Human-in-the-loop Language-agnostic Extraction of Medication Data from Highly Unstructured Electronic Health Records", "pub_year": 2020, "author": "Frank Ruis and Shreyasi Pathak and Jeroen Geerdink and Johannes H Hegeman and Christin Seifert and Maurice van Keulen", "conference": "2020 International Conference on Data Mining Workshops (ICDMW)", "pages": "644-650", "publisher": "IEEE", "abstract": "Electronic health records contain important information written in free-form text. They are often highly unstructured and ungrammatical and contain misspellings and abbreviations, making it difficult to apply traditional natural language processing techniques. Annotated data is hard to come by due to restricted access, and supervised models often don't generalize well to other datasets. We propose a language-agnostic human-in-the-loop approach for extracting medication names from a large set of highly unstructured electronic health records, where we reach almost 97% recall on our test set after the second iteration while maintaining 100% precision. Starting with a bootstrap lexicon we perform a context based dictionary expansion curated by a human reviewer. The method can handle ambiguous lexicon entries and efficiently find fuzzy matches without producing false positives. The human review step ensures a \u2026"}}, {"bib": {"title": "Exploiting Natural Language Processing for Improving Health Processes", "pub_year": 2017, "author": "Maurice Van Keulen and Jeroen Geerdink and Gerard CM Linssen and Riemer HJA Slart and Onno Vijlbrief", "conference": "7th International Symposium on Data-Driven Process Discovery and Analysis (SIMPDA 2017)", "publisher": "CEUR-WS. org", "abstract": "In the medical world, high quality digital registration in an Electronic Patient Dossier (EPD) of symptoms, diagnoses, treatments, test results, images, interpretations, and outcomes becomes commonplace. Together with a shortage of medical professionals, means that they experience pressure at the expense of actual \u2018hands on the bed\u2019. On the other hand, EPDs contain a wealth of largely unused, unstructured textual information. Clinicians primarily communicate with each other through letters and reports. Our main question is: Can Natural Language Processing (NLP) exploit this wealth? By extracting structured data and using it as features for machine learning, a wide variety of process improvements become possible. Furthermore, it may contribute to the desire of government and health stakeholders to simplify registration and relieve pressure. This paper sketches a few prominent process improvements that we plan to research."}}, {"bib": {"title": "MTCB: A Multi-Tenant Customizable database Benchmark", "pub_year": 2017, "author": "Wim van der Zijden and Djoerd Hiemstra and Maurice van Keulen", "conference": "Proceedings of the 9th International Conference on Information Management and Engineering", "pages": "17-23", "publisher": "ACM", "abstract": "We argue that there is a need for Multi-Tenant Customizable OLTP systems. Such systems need a Multi-Tenant Customizable Database (MTC-DB) as a backing. To stimulate the development of such databases, we propose the benchmark MTCB. Benchmarks for OLTP exist and multi-tenant benchmarks exist, but no MTC-DB benchmark exists that accounts for customizability. We formulate seven requirements for the benchmark: realistic, unambiguous, comparable, correct, scalable, simple and independent. It focuses on performance aspects and produces nine metrics: Aulbach compliance, size on disk, tenants created, types created, attributes created, transaction data type instances created per minute, transaction data type instances loaded by ID per minute, conjunctive searches per minute and disjunctive searches per minute. We present a specification and an example implementation in Java 8, which can be \u2026"}}, {"bib": {"title": "Real-time measures of social interaction as predictors for team effectiveness", "pub_year": 2017, "author": "Stijn de Laat and Maaike Dorine Endedijk and Elze Gooitzen Ufkes and Maurice van Keulen and Reinout Everhard de Vries", "conference": "WAOP conference 2017", "abstract": "Real-time measures of social interaction as predictors for team effectiveness \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research \nOutput Datasets Activities Prizes Press / Media Search by expertise, name or affiliation Real-time \nmeasures of social interaction as predictors for team effectiveness Stijn de Laat, Maaike Dorine \nEndedijk, Elze Gooitzen Ufkes, Maurice van Keulen, Reinout Everhard de Vries Educational \nScience Psychology of Conflict, Risk and Safety Databases (Former) Research output: \nContribution to conference \u203a Paper \u203a Academic Overview Original language English Publication \nstatus Published - 2017 Event WAOP conference 2017 - Radboud University, Nijmegen, \nNetherlands Duration: 24 Nov 2017 \u2192 24 Nov 2017 Conference Conference WAOP 2017 City //'\u2026"}}, {"bib": {"title": "Harvesting all matching information to a given query from a deep website", "pub_year": 2015, "author": "Mohammadreza Khelghati and Djoerd Hiemstra and Maurice van Keulen", "publisher": "CEUR-WS. org", "abstract": "In this paper, the goal is harvesting all documents matching a given (entity) query from a deep web source. The objective is to retrieve all information about for instance \u201cDenzel Washington\u201d,\u201cIran Nuclear Deal\u201d, or \u201cFC Barcelona\u201d from data hidden behind web forms. Policies of web search engines usually do not allow accessing all of the matching query search results for a given query. They limit the number of returned documents and the number of user requests. In this work, we propose a new approach which automatically collects information related to a given query from a search engine, given the search engine\u2019s limitations. The approach minimizes the number of queries that need to be sent by applying information from a large external corpus. The new approach outperforms existing approaches when tested on Google, measuring the total number of unique documents found per query."}}, {"bib": {"title": "Incremental Data Uncertainty Handling Using Evidence Combination: A Case Study on Maritime Data Reasoning", "pub_year": 2015, "author": "Mena B Habib and Brend Wanders and Jan Flokstra and Maurice van Keulen", "publisher": "IEEE Computer Society", "abstract": "Semantic incompatibility is a conflict that occurs in the meanings of data. In this paper, we propose an approach for data cleaning by resolving semantic incompatibility. Our approach applies a dynamic and incremental enhancement of data quality. It checks the coherency/conflict of the newly recorded facts/relations against the existing ones. It reasons over the existing information and comes up with new discovered facts/relations. We choose maritime data cleaning as a validation scenario."}}, {"bib": {"title": "Designing A General Deep Web Harvester by Harvestability Factor", "pub_year": 2014, "author": "Mohamamdreza Khelghati and Maurice Van Keulen and Djoerd Hiemstra", "publisher": "CEUR Workshop Proceedings", "abstract": "To make deep web data accessible, harvesters have a crucial role. Targeting different domains and websites enhances the need of a general-purpose harvester which can be applied to different settings and situations. To develop such a harvester, a large number of issues should be addressed. To have all influential elements in one big picture, a new concept, called harvestability factor (HF), is introduced in this paper. The HF is defined as an attribute of a website (HFW) or a harvester (HFH) representing the extent to which the website can be harvested or the harvester can harvest. The comprising elements of these factors are different websites' or harvesters' features. These elements are gathered from literature or introduced through the authors' experiments. In addition to enabling designers of evaluating where they products stand from the harvesting perspective, the HF can act as a framework for designing harvesters. Designers can define the list of features and prioritize their implementations. To validate the effectiveness of HF in practice, it is shown how the HFs elements can be applied in categorizing deep websites and how this is useful in designing a harvester. To validate the HFH as an evaluation metric, it is shown how it can be calculated for the harvester implemented by the authors. The results show that the developed harvester works pretty well for the targeted test set by a score of 14.783 of 15."}}, {"bib": {"title": "Uncertainty Management in Information Systems\u2013Executive Summary by the Organizers", "pub_year": 2009, "author": "Christoph Koch and Birgitta K\u00f6nig-Ries and Volker Markl and Maurice van Keulen", "conference": "Uncertainty Management in Information Systems: Dagstuhl Seminar 08421", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik", "abstract": "Computer science has long pretended that information systems are perfect mirror images of a perfect world. Database management systems, eg, work under the assumption that the data stored represent a correct subset of the real world. Of course, this idealized assumption is rarely true. Information systems contain\u2013wrong information caused, eg, by data entry errors: This is a common problem for instance in genomic databases\u2013imprecise or falsely precise information, eg, a measuring device will provide information with a certain precision only. Typically, information systems store the measured date, but do not store information about the conditions under which this data is true and the precision achieved.\u2013incomplete information. A certain piece of information may not be available to the information system.\u2013inconsistent information. Different information systems may contain contradictory information.In the past information systems have worked around these flaws by extensive consistency checking, plausibility checks, or human discovery and correction. These solutions are bound to fail as systems become ever more distributed, the information more globalized, and the individual systems more autonomous. Hence, we need to find ways for our information systems to directly deal with the uncertainty induced by them."}}, {"bib": {"title": "06472 Abstracts Collection--XQuery Implementation Paradigms", "pub_year": 2007, "author": "Peter A Boncz and Torsten Grust and J\u00e9r\u00f4me Sim\u00e9on and Maurice van Keulen", "conference": "Dagstuhl Seminar Proceedings", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik", "abstract": "From 19.11. 2006 to 22.11. 2006, the Dagstuhl Seminar 06472``XQuery Implementation Paradigms''was held in the International Conference and Research Center (IBFI), Schloss Dagstuhl. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available."}}, {"bib": {"title": "ICT-zakboek", "pub_year": 2007, "author": "Theodorus Maria Aloysius Bemelmans and M van Keulen and RJ Kusters and M Looijen", "publisher": "Reed Business Information", "abstract": "ICT-Zakboek \u2014 University of Twente Research Information Skip to main navigation Skip to \nsearch Skip to main content University of Twente Research Information Logo Home Profiles \nResearch Units Research Output Datasets Activities Prizes Press / Media Search by \nexpertise, name or affiliation ICT-Zakboek TMA Bemelmans (Editor), Maurice van Keulen (Editor), \nRJ Kusters (Editor), M. Looijen (Editor) Databases (Former) Research output: Book/Report \u203a \nBook \u203a Academic Overview Original language Dutch Place of Publication Den Haag Publisher \nReed Business Information Number of pages 926 Edition 3e druk ISBN (Print) 978-90-6228-671-3 \nPublication status Published - Jul 2007 Publication series Name PBNA Polyzakboekjes \nPublisher Reed Business Information No. 2 Keywords METIS-242068 EWI-9513 Cite this APA \nAuthor BIBTEX Harvard Standard RIS Vancouver Bemelmans, TMA, van Keulen, M., Kusters\u2026"}}, {"bib": {"title": "A Syntax-Directed Editor for TM", "pub_year": 1995, "author": "Jochem Vonk and M van Keulen and J Flokstra and H Balsters", "journal": "Department of Computer Science", "pages": "67", "abstract": "This thesis describes the design and implementation of a support tool that will be integrated into the Database Design Tool (DDT). This is an already existing toolset that supports the use of the object-oriented database speci cation language TM. The support tool described in this thesis is a syntax-directed editor (SDE). The purpose of the SDE is to support a user, an unexperienced user in particular, in specifying expressions. The SDE presents a list of syntax constructs that are allowed at speci c places, so the user does not need to know the complete syntax of the language TM. This also prevents the creation of syntactically incorrect expressions. The choice to have a generic structure for the implementation makes the tool language-independent. Although the SDE is implemented to use the language TM it can therefore be easily modi ed to be used with other languages. ii"}}, {"bib": {"title": "Methodological guidelines for IMPRESS", "pub_year": 1995, "author": "F Ardorino and RA de By and R Capobianchi and Maurice van Keulen and M Mautref", "publisher": "Geen opgaven", "abstract": "Methodological Guidelines for IMPRESS (1995) | www.narcis.nl KNAW KNAW Narcis Back \nto search results University of Twente Publication Methodological Guidelines for IMPRESS (1995) \nPagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as \nEndNote Export to RefWorks Title Methodological Guidelines for IMPRESS Author Ardorino, \nF.; de By, RA; Capobianchi, R.; van Keulen, Maurice; Mautref, M. Publisher Databases (Former) \nDate issued 1995 Access Restricted Access Reference(s) METIS-122224 Language und \nType Report Publisher Geen opgaven Publication https://research.utwente.nl/en/publications/methodological-g... \nPersistent Identifier urn:nbn:nl:ui:28-d8f84c8d-e540-4ff5-95f9-2c457b26770d Metadata \nXML Source University of Twente Go to Website Navigation: Home about narcis login \nNederlands contact Anna van Saksenlaan 51 2593 HW Den Haag narcis@dans..nl >\u2026"}}, {"bib": {"title": "IMPRESS Database Design Tool-a high-level design toolset based on formal theory", "pub_year": 1994, "author": "Jan Flokstra and Maurice van Keulen and J Skowronek", "journal": "Exhibition and Demonstration program of the 8th european conference on object-oriented programming (ECOOP), Bologna, Italy, July 4\u20138", "abstract": "This document presents the Database Design Tool prototype, developed at the University of Twente. The Tool is used to specify databases in a graphical way, and is based on a formal specification language TM (described in ECOOP\u201993 article [2]). TM and the Database Design Tool support object-oriented concepts such as classes, object, methods and inheritance. The main point we want to state is that software engineering based on a sound formal basis does not have to sacrifice ease-of-use and flexibility; we state that, on the contrary, it is this formal basis which proves to be beneficial and profitable for the user-enabling faster and error-free software development."}}, {"bib": {"title": "Automatic 3d Building Model Generation Using Deep Learning Methods Based on Cityjson and 2d Floor Plans", "pub_year": 2021, "author": "RG Kippers and M Koeva and M van Keulen and SJ Oude Elberink", "volume": "46", "pages": "49-54", "publisher": "Copernicus GmbH", "abstract": "In the past decade, a lot of effort is put into applying digital innovations to building life cycles. 3D Models have been proven to be efficient for decision making, scenario simulation and 3D data analysis during this life cycle. Creating such digital representation of a building can be a labour-intensive task, depending on the desired scale and level of detail (LOD). This research aims at creating a new automatic deep learning based method for building model reconstruction. It combines exterior and interior data sources: 1) 3D BAG, 2) archived floor plan images. To reconstruct 3D building models from the two data sources, an innovative combination of methods is proposed. In order to obtain the information needed from the floor plan images (walls, openings and labels), deep learning techniques have been used. In addition, post-processing techniques are introduced to transform the data in the required format. In order \u2026"}}, {"bib": {"title": "A Hybrid Text Classification and Language Generation Model for Automated Summarization of Dutch Breast Cancer Radiology Reports", "pub_year": 2020, "author": "Elisa Nguyen and Daphne Theodorakopoulos and Shreyasi Pathak and Jeroen Geerdink and Onno Vijlbrief and Maurice van Keulen and Christin Seifert", "conference": "2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)", "pages": "72-81", "publisher": "IEEE", "abstract": "Breast cancer diagnosis is based on radiology reports describing observations made from medical imagery, such as X-rays obtained during mammography. The reports are written by radiologists and contain a conclusion summarizing the observations. Manually summarizing the reports is time-consuming and leads to high text variability. This paper investigates the automated summarization of Dutch radiology reports. We propose a hybrid model consisting of a language model (encoder-decoder with attention) and a separate BI-RADS score classifier. The summarization model achieved a ROUGE-L F1 score of 51.5% on the Dutch reports, which is comparable to results in other languages and other domains. For the BI-RADS classification, the language model (accuracy 79.1 %) was outperformed by a separate classifier (accuracy 83.3 %), leading us to propose a hybrid approach for radiology report summarization \u2026"}}, {"bib": {"title": "Databases and Information Systems in the AI Era: Contributions from ADBIS, TPDL and EDA 2020 Workshops and Doctoral Consortium", "pub_year": 2020, "author": "Ladjel Bellatreche and Fadila Bentayeb and M\u00e1ria Bielikov\u00e1 and Omar Boussaid and Barbara Catania and Paolo Ceravolo and Elena Demidova and Mirian Halfeld Ferrari and Maria Teresa Gomez Lopez and Carmem S Hara and Slavica Kordi\u0107 and Ivan Lukovi\u0107 and Andrea Mannocci and Paolo Manghi and Francesco Osborne and Christos Papatheodorou and Sonja Risti\u0107 and Dimitris Sacharidis and Oscar Romero and Angelo A Salatino and Guilaine Talens and Maurice van Keulen and Thanasis Vergoulis and Maja Zumer", "pages": "3-20", "publisher": "Springer, Cham", "abstract": "Research on database and information technologies has been rapidly evolving over the last couple of years. This evolution was lead by three major forces: Big Data, AI and Connected World that open the door to innovative research directions and challenges, yet exploiting four main areas: (i) computational and storage resource modeling and organization; (ii) new programming models, (iii) processing power and (iv) new applications that emerge related to health, environment, education, Cultural Heritage, Banking, etc. The 24th East-European Conference on Advances in Databases and Information Systems (ADBIS 2020), the 24th International Conference on Theory and Practice of Digital Libraries (TPDL 2020) and the 16th Workshop on Business Intelligence and Big Data (EDA 2020), held during August 25\u201327, 2020, at Lyon, France, and associated satellite events aimed at covering some emerging issues \u2026"}}, {"bib": {"title": "Data-Driven Process Discovery and Analysis: 8th IFIP WG 2.6 International Symposium, SIMPDA 2018, Seville, Spain, December 13\u201314, 2018, and 9th International Symposium, SIMPDA \u2026", "pub_year": 2020, "author": "Paolo Ceravolo and Maurice van Keulen and Mar\u00eda Teresa G\u00f3mez-L\u00f3pez", "volume": "379", "publisher": "Springer Nature"}}, {"bib": {"title": "Short Term Prediction of Parking Area states Using Real Time Data and Machine Learning Techniques", "pub_year": 2020, "author": "Andreas Kamilaris and Jesper Provoost and Luc Johannes Josephus Wismans and Maurice van Keulen", "conference": "Transportation Research Board (TRB) 99th Annual Meeting", "abstract": "Short Term Prediction of Parking Area states Using Real Time Data and Machine Learning \nTechniques \u2014 University of Twente Research Information Skip to main navigation Skip to \nsearch Skip to main content University of Twente Research Information Logo Home Profiles \nResearch Units Research Output Datasets Activities Prizes Press / Media Search by expertise, \nname or affiliation Short Term Prediction of Parking Area states Using Real Time Data and \nMachine Learning Techniques Andreas Kamilaris, Jesper Provoost, Sander Drift van der, Luc \nJohannes Josephus Wismans, Maurice van Keulen Research output: Contribution to conference \n\u203a Paper \u203a Academic \u203a peer-review Overview Original language English Publication status \nAccepted/In press - 12 Jan 2020 Event Transportation Research Board (TRB) 99th Annual \nMeeting - Walter E. Washington Convention Center, Washinton, United States Duration: 12 Jan : \u2026"}}, {"bib": {"title": "Interactive Explanations of Internal Representations of Neural Network Layers: An Exploratory Study on Outcome Prediction of Comatose Patients", "pub_year": 2020, "author": "Meike Nauta and Michel JAM van Putten and Marleen C Tjepkema-Cloostermans and Jeroen Peter Bos and Maurice van Keulen and Christin Seifert", "abstract": "Supervised machine learning models have impressive predictive capabilities, making them useful to support human decision-making. However, most advanced machine learning techniques, such as Artificial Neural Networks (ANNs), are black boxes and therefore not interpretable for humans. A way of explaining an ANN is visualizing the internal feature representations of its hidden layers (neural embeddings). However, interpreting these visualizations is still difficult. We therefore present InterVENE: an approach that visualizes neural embeddings and interactively explains this visualization, aiming for knowledge extraction and network interpretation. We project neural embeddings in a 2-dimensional scatter plot, where users can interactively select two subsets of data instances in this visualization. Subsequently, a personalized decision tree is trained to distinguish these two sets, thus explaining the difference between the two sets. We apply InterVENE to a medical case study where interpretability of decision support is critical: outcome prediction of comatose patients. Our experiments confirm that InterVENE can successfully extract knowledge from an ANN, and give both domain experts and machine learning experts insight into the behaviour of an ANN. Furthermore, InterVENE\u2019s explanations about outcome prediction of comatose patients seem plausible when compared to existing neurological domain knowledge."}}, {"bib": {"title": "Scalable Interdisciplinary Data Science Teaching at the University of Twente", "pub_year": 2020, "author": "Maurice van Keulen and Christin Seifert and Mannes Poel and C Oudshoorn", "journal": "Berlin Journal of Data Science", "volume": "1", "abstract": "Data scientists are in high demand in many disciplines and domains. This paper describes the data science course open to all master students of the University of Twente. We outline the main challenges of teaching a large and heterogeneous population of non-computer science students about data science and how we addressed them, as well as a historical perspective on how the course grew and evolved."}}, {"bib": {"title": "Data-Driven Process Discovery and Analysis SIMPDA 2017", "pub_year": 2017, "author": "Paolo Ceravolo and Maurice Van Keulen and Kilian Stoffel", "abstract": "Data-Driven Process Discovery and Analysis SIMPDA 2017 Page 1 7thInternational \nSymposium on Data-Driven Process Discovery and Analysis SIMPDA 2017 December 6-8, \n2017 Neuchatel, Switzerland Editors: Paolo Ceravolo Maurice Van Keulen Kilian Stoffel \nPage 2 II Foreword With the increasing automation of business processes, growing \namounts of process data become available. This opens new research opportunities for \nbusiness process data analysis, mining, and modeling. The aim of the IFIP 2.6 - \nInternational Symposium on Data- Driven Process Discovery and Analysis is to offer a forum \nwhere researchers from different communities and the industry can share their insight in this \nhot new field. Submissions aim at covering theoretical issues related to process \nrepresentation, discovery and analysis, or provide practical and operational experiences in \nprocess discovery and analysis. Language for papers \u2026"}}, {"bib": {"title": "Evidence combination for incremental decision-making processes", "pub_year": 2016, "author": "Ghita Berrada and Maurice van Keulen and Ander de Keijzer", "journal": "CTIT technical report", "number": "CTIT-TR-16-01", "publisher": "University of Twente, Centre for Telematica and Information Technology (CTIT)", "abstract": "The establishment of a medical diagnosis is an incremental process highly fraught with uncertainty. At each step of this painstaking process, it may be beneficial to be able to quantify the uncertainty linked to the diagnosis and steadily update the uncertainty estimation using available sources of information, for example user feedback, as they become available. Using the example of medical data in general and EEG data in particular, we show what types of evidence can affect discrete variables such as a medical diagnosis and build a simple and computationally efficient evidence combination model based on the Dempster-Shafer theory."}}, {"bib": {"title": "NLPIT 2016 Chairs\u2019 Welcome", "pub_year": 2015, "author": "Mena B Habib and Florian Kunneman and Maurice van Keulen", "abstract": "It is our great pleasure to welcome you to the 2nd International Workshop on Natural Language Processing for Informal Text (NLPIT), associated with WWW 2016."}}, {"bib": {"title": "Dealing with poor data quality of OSINT data in fraud risk analysis", "pub_year": 2015, "author": "Maurice van Keulen", "publisher": "Tilburg University", "abstract": "Governmental organizations responsible for keeping certain types of fraud under control, often use data-driven methods for both immediate detection of fraud, or for fraud risk analysis aimed at more effectively targeting inspections. A blind spot in such methods, is that the source data often represents a'paper reality'. Fraudsters will attempt to disguise themselves in the data they supply painting a world in which they do nothing wrong. This blind spot can be counteracted by enriching the data with traces and indicators from more'real-world'sources such as social media and internet. One of the crucial data management problems in accomplishing this enrichment is how to capture and handle data quality problems. The presentation will start with a real-world example, which is also used as starting point for a problem generalization in terms of information combination and enrichment (ICE). We then present the ICE technology as well as how data quality problems can be managed with probabilistic databases. In terms of the 4 V's of big data--volume, velocity, variety and veracity--this presentation focuses on the third and fourth V's: variety and veracity."}}, {"bib": {"title": "Stairwalker user manual", "pub_year": 2015, "author": "Dennis Muller and Jochem Elsinga and Maurice Keulen", "number": "TR-CTIT-15-09", "publisher": "University of Twente, Centre for Telematics and Information Technology (CTIT)", "abstract": "Geographical data are typically visualized using various information layers that are displayed over a map. Interactive exploration by zooming and panning actions needs real-time re-calculation. A common operation in calculating with multidimensional data is the computation of aggregates. For layers containing aggregated information derived from voluminous data sets, such real-time exploration is impossible using standard database technology. Calculations require too much time. The University of Twente has developed \u201cStairwalker\u203f: database technology that accurately aggregates data so that they can geographically be explored in real-time. The technology is a plug-in to common open source technology. Its core is the pre-aggregate index: a database index that cleverly precalculates aggregation values such that it can obtain exact aggregation results from voluminous data with high performance. A fast calculation allows to fully recalculate the result for even the slightest movement of the map, such as a panning or zooming action, without loss of accuracy. Thanks to this indexing mechanism, we can provide a scalable real-time calculation: an order of magnitude larger dataset requires only one additional aggregation level. In geo data visualization, the ability to quickly develop new information layers is important. Although many solutions exist, there is a niche: the combination of visualizing aggregation information, interactive data exploration in real-time, Big Data, calculating exact numbers instead of approximations, and doing so with common open source technology. Our technology for the first time integrates all these features. Our \u2026"}}, {"bib": {"title": "Towards Online Relational Schema Transformations", "pub_year": 2014, "author": "Lesley Wevers and M Hofstra and M Tammens and M Huisman and M van Keulen", "publisher": "Dutch-Belgian Database Day", "abstract": "Current relational database systems are ill-equipped for changing the structure of data while the database is in use. This is a real problem for systems for which we expect 24/7 availability, such as telecommunication, payment, and control systems. As a result, developers tend to avoid making changes because of the downtime consequences. The urgency to solve this problem is evident by a multitude of tools developed in industry, such as pt-online-schema-change1 and oak-online-alter-table2. Also, MySQL recently added limited support for online schema changes3. Contributions: We want to draw the attention of the database community to the problem of online schema changes. We have defined requirements for online schema change mechanisms, and we have experimentally investigated existing solutions. Our results show that current solutions are unsatisfactory for complex schema changes. We propose lazy schema changes as a solution. Experimental Setup: To assess the performance and behaviour of existing mechanisms for on-line schema changes, we have developed an experiment based on the standard TPC-C benchmark. For each of the relational schema transformation classes that we have identified, we chose a rep-resentative transformation for the TPC-C schema. We perform the schema change online while the TPC-C benchmark is running, and measure the impact on the TPC-C transaction through-put. We have performed our experiment on PostgreSQL, which does not support online schema changes, MySQL, which supports basic online schema changes, and using pt-online-schema-change on MySQL, as a \u2026"}}, {"bib": {"title": "Finding You on the Internet: an approach for finding on-line presences of people for fraud risk analysis", "pub_year": 2014, "author": "Henry Been and Maurice van Keulen", "conference": "16th International Conference on Enterprise Information Systems (ICEIS 2014)", "pages": "697-706", "publisher": "SciTePress"}}, {"bib": {"title": "How much data resides in a web collection: how to estimate size of a web collection", "pub_year": 2013, "author": "Mohammadreza Khelghati and Djoerd Hiemstra and Maurice Van Keulen", "publisher": "CEUR", "abstract": "With increasing amount of data in deep web sources (hidden from general search engines behind web forms), accessing this data has gained more attention. In the algorithms applied for this purpose, it is the knowledge of a data source size that enables the algorithms to make accurate decisions in stopping crawling or sampling processes which can be so costly in some cases [4]. The tendency to know the sizes of data sources is increased by the competition among businesses on the Web in which the data coverage is critical. In the context of quality assessment of search engines [2], search engine selection in the federated search engines, and in the resource/collection selection in the distributed search field [6], this information is also helpful. In addition, it can give an insight over some useful statistics for public sectors like governments. In any of these mentioned scenarios, in case of facing a non-cooperative collection which does not publish its information, the size has to be estimated [5]. In this paper, the approaches in literature are categorized and reviewed. The most recent approaches are implemented and compared in a real environment. Finally, four methods based on the modification of the available techniques are introduced and evaluated. In one of the modifications, the estimations from other approaches could be improved ranging from 35 to 65 percent.Contributions. As the first contribution, an experimental comparison among a number of size estimation approaches is performed. Having applied these techniques on a number of real search engines, it is shown which technique can provide more promising results. As the second \u2026"}}, {"bib": {"title": "//Rondje Zilverling: COMMIT/TimeTrails", "pub_year": 2013, "author": "Maurice van Keulen and Victor Graaff and Zhemin Zhu and Andreas de By and Rolf and Wombacher and Jan Flokstra", "journal": "I/O Vivat", "volume": "28", "number": "4", "pages": "16-17", "publisher": "Inter-Actief, University of Twente", "abstract": "Het TimeTrails-project3 gaat over data mining in grote hoeveelheden gegevens over gebeurtenissen in ruimte en tijd, dwz met co\u00f6rdinaten en time-stamps. Dergelijke gegevens worden doorgaans vergaard door mensen, sensoren en wetenschappelijke observaties. Gegevensanalyse richt zich vaak op de vier W\u2019s: Wie, Wat, Waar en Wanneer. Een belangrijke kwestie is het kunnen behappen van de grote hoeveelheden gegevens, dwz\" big data\". Vanuit de UT werken we, dwz de groepen EWI/DB en ITC/GIP, aan twee applicaties:* Het in kaart brengen van de mening van het publiek bij grote infrastructuurproject zoals de aanleg van een nieuw stuk snelweg. Dit doen we met Twitter-analyse en data-visualisatie.\u2022 Het vinden van goede vakantiebestemmingen. Hierbij spelen Social media, web harvesting en analyse van GPS-traces een rol."}}, {"bib": {"title": "Semantic Enrichment of GPS Trajectories", "pub_year": 2012, "author": "Victor de Graaff and Maurice van Keulen and RA de By", "publisher": "University of Brussels", "abstract": "Semantic annotation of GPS trajectories helps us to recognize the interests of the creator of the GPS trajectories. Automating this trajectory annotation circumvents the requirement of additional user input. To annotate the GPS traces automatically, two types of automated input are required: 1) a collection of possible annotations, and 2) a collection of GPS trajectories to annotate. The first type of input can be a set of points of interest (POIs), activities, weather types, etc. This collection is to be provided by an application developer, and can originate from the web, an external knowledge base, or an existing database, for example. The type of annotation that we are interested in, is annotation with visited locations, in order to create a user profile at a later stage. We have collected POIs by scraping the web, using a self-configuring data harvester. This harvester is based on workflows, enabling us to add or remove certain steps for different goals of harvesting. The result of our harvesting approach consists of a set of 27,384 POIs, origining from the Dutch Yellow Pages\\cite {goudengids2012, and contains an address and a geographical point representation for each POI. These point representations cannot be used to overlay the GPS trajectories directly, and therefore need to be converted into a polygon before providing useful input for the annotation process. Several different approaches to this problem can be thought of, including Voronoi diagrams, nearest-neighbors, and geocoding the addresses of the assumed neighbors. For each of the POI footprint size estimation approaches, the output consists of two parts: 1) a polygon representing the estimated \u2026"}}, {"bib": {"title": "Named Entity Extraction and Disambiguation from an Uncertainty Perspective", "pub_year": 2011, "author": "Mena Badieh Habib and Maurice van Keulen", "pages": "12", "publisher": "Centre for Telematics and Information Technology (CTIT)", "abstract": "Named entity extraction and disambiguation have received much attention in recent years. Typical fields addressing these topics are information retrieval, natural language processing, and semantic web. This work addresses two problems with named entity extraction and disambiguation. First, almost no existing works examine the extraction and disambiguation interdependency. Second, existing disambiguation techniques mostly take as input extracted named entities without considering the uncertainty and imperfection of the extraction process. It is the aim of this work to investigate both avenues and to show that explicit handling of the uncertainty of annotation has much potential for making both extraction and disambiguation more robust. We conducted experiments with a set of holiday home descriptions with the aim to extract and disambiguate toponyms as a representative example of named entities. We show that the effectiveness of extraction influences the effectiveness of disambiguation, and reciprocally, how retraining the extraction models with information automatically derived from the disambiguation results, improves the extraction models. This mutual reinforcement is shown to even have an effect after several iterations."}}, {"bib": {"title": "Integration of Biological Sources: Exploring the Case of Protein Homology", "pub_year": 2011, "author": "Tjeerd W Boerman and Maurice Van Keulen and Paul Van Der Vet and Edouard I Severing", "journal": "CTIT Technical Report Series", "number": "TR-CTIT-11-18", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "Data integration is a key issue in the domain of bioinformatics, which deals with huge amounts of heterogeneous biological data that grows and changes rapidly. This paper serves as an introduction in the field of bioinformatics and the biological concepts it deals with, and an exploration of the integration problems a bioinformatics scientist faces. We examine ProGMap, an integrated protein homology system used by bioinformatics scientists at Wageningen University, and several use cases related to protein homology. A key issue we identify is the huge manual effort required to unify source databases into a single resource. Uncertain databases are able to contain several possible worlds, and it has been proposed that they can be used to significantly reduce initial integration efforts. We propose several directions for future work where uncertain databases can be applied to bioinformatics, with the goal of furthering the cause of bioinformatics integration."}}, {"bib": {"title": "Proceedings of the Fifth International VLDB Workshop on Management of Uncertain Data (MUD)", "pub_year": 2011, "author": "Ander Keijzer and Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology"}}, {"bib": {"title": "The Fourth International VLDB Workshop on Management of Uncertain Data", "pub_year": 2010, "author": "A de Keijzer and M van Keulen", "number": "WP10-04", "publisher": "Centre for Telematics and Information Technology University of Twente"}}, {"bib": {"title": "Working Group: Classification, Representation and Modeling", "pub_year": 2009, "author": "Anish Das Sarma and Ander de Keijzer and Amol Deshpande and Peter J Haas and Ihab F Ilyas and Christoph Koch and Thomas Neumann and Dan Olteanu and Martin Theobald and Vasilis Vassalos", "conference": "Dagstuhl Seminar on Uncertainty Management in Information Systems 2009", "publisher": "Dagstuhl", "abstract": "This report briefly summarizes the discussions carried out in the working group on classification, representation and modeling of uncertain data. The discussion was divided into two subgroups: the first subgroup studied how different representation and modeling alternatives currently proposed can fit in a bigger picture of theory and technology interaction, while the second subgroup focused on contrasting current system implementations and the reasons behind such diverse class of available prototypes. We summarize the findings of these two groups and the future steps suggested by group members."}}, {"bib": {"title": "Proceedings of the Third International Workshop on Management of Uncertain Data (MUD2009)", "pub_year": 2009, "author": "Ander de Keijzer and Maurice van Keulen", "number": "WP-CTI", "publisher": "Centre for Telematics and Information Technology, University of Twente", "abstract": "Proceedings of the Third International Workshop on Management of... (2009) | www.narcis.nl \nKNAW KNAW Narcis Back to search results University of Twente Publication Proceedings of the \nThird International Workshop on Management of... (2009) Pagina-navigatie: Main Save \npublication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title \nProceedings of the Third International Workshop on Management of Uncertain Data (MUD2009) \nSeries CTIT Workshop Proceedings Series. Centre for Telematics and Information Technology \nUniversity of Twente Author de Keijzer, Ander; van Keulen, Maurice Publisher Databases \n(Former); Faculty of Science and Technology Date issued 2009-08-28 Access Restricted Access \nReference(s) METIS-265226, IR-69886, EWI-16023 Language English Type Book Publisher \nCentre for Telematics and Information Technology (CTIT) Publication https://research..//\u2026"}}, {"bib": {"title": "Probabilistic Data Integration", "pub_year": 2009, "author": "C Koch and B K\u00f6nig-Ries and V Markl and M van Keulen", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik"}}, {"bib": {"title": "Imprecision, Diversity and Uncertainty: Disentangling Threads in Uncertainty Management: 08421 Working Group", "pub_year": 2009, "author": "M Spiliopoulou and M Keulen and H-J Lenz and J Wijsen and M Renz and R Kruse and M Stern", "number": "08421", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik"}}, {"bib": {"title": "08421 Working Group: Report of the Probabilistic Databases Benchmarking", "pub_year": 2009, "author": "Christoph Koch and C Re and D Olteanu and H-J Lenz and M van Keulen and PJ Haas and Jeff Z Pan", "number": "08421", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik", "abstract": "08421 Working Group: Report of the Probabilistic Databases Benchmarking \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research Output \nDatasets Activities Prizes Press / Media Search by expertise, name or affiliation 08421 Working \nGroup: Report of the Probabilistic Databases Benchmarking C. Koch, C. Re, D. Olteanu, HJ Lenz, \nPJ Haas, JZ Pan Databases (Former) Research output: Chapter in Book/Report/Conference \nproceeding \u203a Conference contribution \u203a Academic 20 Downloads (Pure) Overview Fingerprint \nAbstract The results of the probabilistic database benchmark working group. Original \nlanguage Undefined Title of host publication Proceedings of Dagstuhl Seminar 08421 on \nUncertainty Management in Information Systems Editors C. Koch, B. K\u00f6nig-Ries, V. Markl, , (\u2026"}}, {"bib": {"title": "08421 Working Group: Imprecision, Diversity and Uncertainty: Disentangling Threads in Uncertainty Management", "pub_year": 2009, "author": "Myra Spiliopoulou and Maurice van Keulen and Hans-Joachim Lenz and Jef Wijsen and Matthias Renz and Rudolf Kruse and Mirco Stern", "conference": "Dagstuhl Seminar Proceedings", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik", "abstract": "We report on the results of Workgroup 1 on\" Imprecision, Diversity and Uncertainty\". We set the scene by elaborating on where uncertainty comes from and what the ground truth is. In real world applications, the data observed may not be as expected: they may violate constraints, or, more generally, disagree with the anticipated model of the world. This leads to two orthogonal cases: The data may be erroneous, ie they must be corrected. Or, the model may outdated and must be adjusted to the data. After elaborating on this fundamental distinction, we address the issues of measuring uncertainty and exploiting uncertainty in real applications. We conclude with a list of challenges that should be addressed when dealing with uncertainty."}}, {"bib": {"title": "Proceedings of the International Workshop on Quality in Databases and Management of Uncertain Data (QDBMUD2008)", "pub_year": 2008, "author": "Ander de Keijzer and Maurice van Keulen and P Missier and X Lin", "publisher": "Centre for Telematics and Information Technology (CTIT)", "abstract": "The ability to detect and correct errors in the data, and more broadly to develop techniques for data quality assessment, has long been recognized as critical to the functionality of a large number of applications, in areas ranging from business management to data-intensive science. While many of the technical issues associated with data quality have been known for quite some time, novel applications still pose original challenges, while advances in data management technology offer ideas for novel approaches. The sixth in a workshop series dedicated specifically to problems of Quality in Databases, QDB'08 is a qualified forum for presenting and discussing novel ideas and solutions related to the problems of assessing, monitoring, improving, and maintaining the quality of data. Previous editions of the workshop were co-located with top-level data management conferences, namely SIGMOD and VLDB. The workshop on Management of Uncertain Data (MUD) is the third in a row addressing the area of techniques for handling uncertainty in data. The first workshop took place at the University of Twente in 2006 in the Twente Data Management workshop series and last year the workshop was also co-located with VLDB. This year, QDB and MUD have joined forces and have been able to offer a rich and qualified program, consisting of 12 original research papers, each subject to the scrutiny of at least three reviewers, and an invited talk. The workshop structure reflects three main areas of interest from the community. The first session addresses traditional issues of Record Linkage and Data Correction using distinctly novel techniques. The second \u2026"}}, {"bib": {"title": "Proceedings of the first international VLDB workshop on Management of Uncertain Data", "pub_year": 2007, "author": "Ander de Keijzer and Maurice van Keulen and Alex Dekhtyar", "number": "WP-CTI", "publisher": "Centre for Telematics and Information Technology", "abstract": "Proceedings of the first international VLDB workshop on Management of Uncertain Data (2007) \n| www.narcis.nl KNAW KNAW Narcis Back to search results University of Twente Publication \nProceedings of the first international VLDB workshop on Management of Uncertain Data (2007) \nPagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote \nExport to RefWorks Title Proceedings of the first international VLDB workshop on Management \nof Uncertain Data Series Proceedings Workshop Series. Centre for Telematics and \nInformation Technology University of Twente Author de Keijzer, Ander; van Keulen, Maurice; \nDekhtyar, A. Publisher Databases (Former); Faculty of Science and Technology Date issued \n2007-09-24 Access Restricted Access Reference(s) METIS-241987, EWI-11233, IR-64412 \nLanguage und Type Book Publisher Centre for Telematics and Information Technology () :/\u2026"}}, {"bib": {"title": "Gegevensbanken", "pub_year": 2007, "author": "TMA Bemelmans and M van Keulen and RJ Kusters and M Looijen", "journal": "ICT-Zakboek", "pages": "852-881", "publisher": "Reed Business Information"}}, {"bib": {"title": "Computerondersteuning voor groepscommunicatie en samenwerken", "pub_year": 2007, "author": "M van Keulen and K Sikkel", "publisher": "Reed Business Information"}}, {"bib": {"title": "Ch. VI. 2 Programmatuur", "pub_year": 2007, "author": "A van Deursen and P Klint", "pages": "69-108", "publisher": "Reed Business Information"}}, {"bib": {"title": "Computerondersteuning voor groepscommunicatie en samenwerken", "pub_year": 2007, "author": "Maurice Keulen and Klaas Sikkel", "pages": "341-350", "publisher": "Reed Business Information"}}, {"bib": {"title": "Data warehousing en data mining", "pub_year": 2007, "author": "APJM Siebes and M van Keulen and TMA Bemelmans and RJ Kusters and M Looijen", "journal": "ICT-Zakboek", "pages": "332-341", "publisher": "Reed Business Information", "abstract": "Data warehousing en data mining (2007) | www.narcis.nl KNAW KNAW Narcis Back to search \nresults University of Twente Publication Data warehousing en data mining (2007) \nPagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote \nExport to RefWorks Title Data warehousing en data mining Published in ICT-Zakboek, 332 - 341 \nSeries PBNA Polyzakboekjes. Reed Business Information Author Siebes, APJM Editor \nBemelmans, TMA; van Keulen, Maurice; Kusters, RJ; Looijen, M. Publisher Databases (Former) \nDate issued 2007-7 Access Restricted Access Reference(s) EWI-9509, IR-63972, METIS-242065 \nLanguage und Type Book Part Publisher Reed Business Information Publication \nhttps://research.utwente.nl/en/publications/data-warehousing... OpenURL Search this publication \nin (your) library ISBN 978-90-6228-671-3 Persistent Identifier urn:nbn:nl:ui:28-63972 Metadata of .\u2026"}}, {"bib": {"title": "Procesautomatisering", "pub_year": 2007, "author": "M van Keulen and TMA Bemelmans and RJ Kusters and M Looijen", "journal": "ICT-Zakboek", "pages": "212-222", "publisher": "Reed Business Information", "abstract": "Procesautomatisering (2007) | www.narcis.nl KNAW KNAW Narcis Back to search results \nUniversity of Twente Publication Procesautomatisering (2007) Pagina-navigatie: Main Save \npublication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title \nProcesautomatisering Published in ICT-Zakboek, 212 - 222 Series PBNA Polyzakboekjes. \nReed Business Information Author van Keulen, Maurice Editor Bemelmans, TMA; van \nKeulen, Maurice; Kusters, RJ; Looijen, M. Publisher Databases (Former) Date issued 2007-7 \nAccess Restricted Access Reference(s) EWI-9508, IR-63971, METIS-242064 Language \nDutch Type Book Part Publisher Reed Business Information Publication https://research.utwente.nl/en/publications/procesautomatise... \nOpenURL Search this publication in (your) library ISBN 978-90-6228-671-3 Persistent \nIdentifier urn:nbn:nl:ui:28-63971 Metadata XML Source University of Go : \u2026"}}, {"bib": {"title": "06472 Executive Summary--XQuery Implementation Paradigms", "pub_year": 2007, "author": "Peter A Boncz and Torsten Grust and J\u00e9r\u00f4me Sim\u00e9on and Maurice van Keulen", "conference": "Dagstuhl Seminar Proceedings", "publisher": "Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik", "abstract": "Only a couple of weeks after the participants of seminar No. 06472 met in Dagstuhl, the W3C published the Final Recommendation documents that fix the XQuery 1.0 syntax, data model, formal semantics, built-in function library and the interaction with the XML Schema Recommendations (see W3C's XQuery web site at http://www. w3. org/XML/Query/). With the language's standardization nearing its end and now finally in place, the many efforts to construct correct, complete, and efficient implementations of XQuery finally got rid of the hindering\" moving target''syndrome. This Dagstuhl seminar on the different XQuery implementation paradigms that have emerged in the recent past, thus was as timely as it could have possibly been."}}, {"bib": {"title": "Proceedings of the 2nd Twente Data Management Workshop (TDM'06) on Uncertainty in Databases", "pub_year": 2006, "author": "Ander de Keijzer and Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology University of Twente"}}, {"bib": {"title": "Assignments for XML&DB 2 Theme \u201cProbabilistic XML\u201d", "pub_year": 2006, "author": "Maurice van Keulen and Ander de Keijzer and Djoerd Hiemstra", "abstract": "Data integration is a challenging problem in many application areas. Approaches mostly attempt to resolve semantic uncertainty and conflicts between information sources as part of the data integration process. In some application areas this is impractical or even prohibitive, for example, in an ambient environment where devices on an ad hoc basis have to exchange information autonomously, managing and resolving data conflicts and semantical uncertainty without interference from a user. It is the task of the data management system of the device to support this. A probabilistic approach seems promising as it does not require extensive semantic annotations nor user interaction at integration time. It simply teaches the application how to generically cope with uncertainty. In probabilistic XML, the integrated information source represents all possible appearances of objects in the real world, the so-called possible \u2026"}}, {"bib": {"title": "Optimization of Pathfinder Queries (Opaque)[public version]", "pub_year": 2006, "author": "Maurice van Keulen", "abstract": "The Opaque project focuses on query optimization for relational XQuery engines. Advances in techniques for storage and query processing for XML using existing relational engines show that this approach has great potential for being able to manage the ever growing volumes of XML data. Many storage models, index structures, and algorithms have been proposed to speed up queries. Currently, attention is shifting towards query optimization techniques to address the remaining performance problems, to which this research hopes to make a contribution."}}, {"bib": {"title": "Rule-based Information Integration", "pub_year": 2005, "author": "Ander de Keijzer and Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology (CTIT)", "abstract": "In this report, we show the process of information integration. We specifically discuss the language used for integration. We show that integration consists of two phases, the schema mapping phase and the data integration phase. We formally define transformation rules, conversion, evolution and versioning. We further discuss the integration process from a data point of view."}}, {"bib": {"title": "Third Workshop on Ambient Databases", "pub_year": 2005, "author": "P Boncz and Maurice van Keulen and AH van Bunningen", "abstract": "Third Workshop on Ambient Databases \u2014 University of Twente Research Information Skip to \nmain navigation Skip to search Skip to main content University of Twente Research Information \nLogo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media \nSearch by expertise, name or affiliation Third Workshop on Ambient Databases P. Boncz, \nMaurice van Keulen, AH van Bunningen Databases (Former) Research output: Other \ncontribution \u203a Other research output Overview Original language Undefined Place of Publication \nAmsterdam, The Netherlands Publication status Published - 1 Nov 2005 Keywords \nMETIS-225834 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Boncz, P., van \nKeulen, M., & van Bunningen, AH (2005, Nov 1). Third Workshop on Ambient Databases. Boncz, \nP. ; van Keulen, Maurice ; van Bunningen, AH / Third Workshop on Ambient Databases. 2005. , . \u2026"}}, {"bib": {"title": "Organisatie Dutch-Belgian Database Day", "pub_year": 2005, "author": "P Boncz and Maurice van Keulen and AH van Bunningen", "abstract": "Organisatie Dutch-Belgian Database Day (2005) | www.narcis.nl KNAW KNAW Narcis Back to \nsearch results University of Twente Publication Organisatie Dutch-Belgian Database Day \n(2005) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as \nEndNote Export to RefWorks Title Organisatie Dutch-Belgian Database Day Author Boncz, P.; \nvan Keulen, Maurice; van Bunningen, AH Publisher Databases (Former) Date issued \n2005-10-31 Access Restricted Access Reference(s) METIS-225802 Language und Type Other \nPublication https://research.utwente.nl/en/publications/organisatie-dutc... Persistent Identifier \nurn:nbn:nl:ui:28-08329270-c28f-4518-80f4-04b12143891f Metadata XML Source University of \nTwente Go to Website Navigation: Home about narcis login Nederlands contact Anna van \nSaksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl More >>> Youtube Newsletter >>> >\u2026"}}, {"bib": {"title": "MonetDB/XQuery, Technology preview open source release 1", "pub_year": 2005, "author": "P Boncz and S Manegold and Sjoerd Mullender and Maurice van Keulen and Jan Flokstra and T Grust and J Teubner and J Rittinger", "abstract": "MonetDB/XQuery, Technology preview open source release 1 \u2014 University of Twente \nResearch Information Skip to main navigation Skip to search Skip to main content University of \nTwente Research Information Logo Home Profiles Research Units Research Output Datasets \nActivities Prizes Press / Media Search by expertise, name or affiliation MonetDB/XQuery, \nTechnology preview open source release 1 P. Boncz, S. Manegold, Sjoerd Mullender, Maurice \nvan Keulen, Jan Flokstra, T. Grust, J. Teubner, J. Rittinger Databases (Former) Research output: \nOther contribution \u203a Other research output Overview Original language Undefined Place of \nPublication Amsterdam, The Netherlands Publication status Published - 15 Jun 2005 Keywords \nMETIS-225881 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Boncz, P., \nManegold, S., Mullender, S., van Keulen, M., Flokstra, J., Grust, T., Teubner, J., & Rittinger, J. (\u2026"}}, {"bib": {"title": "Advanced SIKS-course XML: where databases and information retrieval meet", "pub_year": 2005, "author": "Djoerd Hiemstra and Maurice van Keulen", "publisher": "SIKS Research School", "abstract": "Advanced SIKS-course XML: where databases and information retrieval meet \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research \nOutput Datasets Activities Prizes Press / Media Search by expertise, name or affiliation \nAdvanced SIKS-course XML: where databases and information retrieval meet Djoerd Hiemstra, \nMaurice van Keulen Databases (Former) Research output: Other contribution \u203a Other research \noutput Overview Original language Undefined Publisher SIKS Research School Place of \nPublication Leusden, The Netherlands Publication status Published - 18 Apr 2005 Keywords \nMETIS-225836 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Hiemstra, D., & \nvan Keulen, M. (2005, Apr 18). Advanced SIKS-course XML: where databases and information . . \u2026"}}, {"bib": {"title": "Second Multimedian Workshop on Ambient Multimedia Databases", "pub_year": 2004, "author": "Maurice van Keulen", "abstract": "Second Multimedian Workshop on Ambient Multimedia Databases (2004) | www.narcis.nl \nKNAW KNAW Narcis Back to search results University of Twente Publication Second \nMultimedian Workshop on Ambient Multimedia Databases (2004) Pagina-navigatie: Main \nSave publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks \nTitle Second Multimedian Workshop on Ambient Multimedia Databases Author van Keulen, \nMaurice Publisher Databases (Former) Date issued 2004-10-14 Access Restricted Access \nReference(s) METIS-222027 Language und Type Other Publisher University of Twente \nPublication https://research.utwente.nl/en/publications/second-multimedi... Persistent \nIdentifier urn:nbn:nl:ui:28-64edcf71-bde1-4200-9d5d-6204e5697a75 Metadata XML \nSource University of Twente Go to Website Navigation: Home about narcis login \nNederlands contact Anna van Saksenlaan 51 ..\u2026"}}, {"bib": {"title": "Seventh EDBT Summper School on XML & Databases", "pub_year": 2004, "author": "Maurice van Keulen", "abstract": "Seventh EDBT Summper School on XML & Databases \u2014 University of Twente Research \nInformation Skip to main navigation Skip to search Skip to main content University of Twente \nResearch Information Logo Home Profiles Research Units Research Output Datasets Activities \nPrizes Press / Media Search by expertise, name or affiliation Seventh EDBT Summper School \non XML & Databases Maurice van Keulen Databases (Former) Research output: Other \ncontribution \u203a Other research output Overview Original language Undefined Place of Publication \nSardinia, Italy Publication status Published - 6 Sep 2004 Keywords METIS-222032 Cite this \nAPA Author BIBTEX Harvard Standard RIS Vancouver van Keulen, M. (2004, Sep 6). Seventh \nEDBT Summper School on XML & Databases. van Keulen, Maurice. / Seventh EDBT Summper \nSchool on XML & Databases. 2004. Sardinia, Italy. van Keulen, M 2004, Seventh EDBT . \u2026"}}, {"bib": {"title": "Zoeken in XML: van XPath naar SQL", "pub_year": 2004, "author": "H van Rein and Maurice van Keulen", "journal": "DB/M: database magazine", "number": "06", "pages": "45-49", "abstract": "Zoeken in XML: van XPath naar SQL (2004) | www.narcis.nl KNAW KNAW Narcis Back to \nsearch results University of Twente Publication Zoeken in XML: van XPath naar SQL (2004) \nPagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as \nEndNote Export to RefWorks Title Zoeken in XML: van XPath naar SQL Published in \nDatabase magazine, 45 - 49. SDU. ISSN 0925-6911. Author van Rein, H.; van Keulen, \nMaurice Publisher Databases (Former) Date issued 2004 Access Restricted Access \nReference(s) METIS-221349 Language und Type Article Publisher SDU Publication https://research.utwente.nl/en/publications/zoeken-in-xml-va... \nOpenURL Search this publication in (your) library Persistent Identifier urn:nbn:nl:ui:28-52e14590-b379-4a36-a2f4-a001fb5659cf \nMetadata XML Source University of Twente Go to Website Navigation: Home about narcis \nlogin Nederlands contact Anna van \u2026"}}, {"bib": {"title": "Perspective on NF2", "pub_year": 2003, "author": "M van Keulen and J Vonk and AP de Vries and J Flokstra and HE Blok", "journal": "Database and Expert Systems Applications", "pages": "67", "publisher": "Springer-Verlag", "abstract": "Advanced non-traditional application domains such as geographic in-formation systems and digital library systems demand advanced data management support. In an effort to cope with this demand, we present the concept of a novel multi-model DBMS architecture which provides evaluation of queries on complexly structured data without sacrificing efficiency. A vital role in this architecture is played by the Moa language featuring a nested relational data model based on"}}, {"bib": {"title": "De hamer of de moker: welke databasesoftware te kiezen", "pub_year": 2002, "author": "Maurice van Keulen and A Offerman", "journal": "Linux news", "volume": "3", "number": "1", "pages": "14-21", "abstract": "Voor serversystemen is Linux als operating system zeer geschikt. Menig webserver wordt gehost op een Linux-gebaseerd systeem en steeds meer organisaties kiezen voor Linux voor hun back-office. Een essentiele component daarbij is het database management system (DBMS). Het DBMS draagt zorg voor het beheer van uw kostbare gegevens. Decennialange ervaring met DBMS'en, die teruggaat tot de eerste mainframes, heeft tot producten geleid waarin die gegevens veilig en betrouwbaar opgeslagen liggen en waarmee u er snelle toegang tot hebt. Voor het Linux-platform zijn zowel open-source DBMS'en beschikbaar, zoals MySQL en PostgreSQL, als Linux-versies van bekende commerciele producten, zoals Oracle en IBM DB2. Er bestaat geen eenduidige winnaar van de prijs voor het beste DBMS voor Linux. Er is altijd een trade-off die u zult moeten afwegen. Dit artikel zal u helpen een weloverwogen keuze te maken voor het product-de hamer of de moker-dat het beste bij uw toepassing past."}}, {"bib": {"title": "Report on the development issues: Techniques", "pub_year": 2002, "author": "Maurice van Keulen and J Vonk", "journal": "Deliverable D6 of the SUMMER project", "publisher": "Databases (DB)", "abstract": "Report on the development issues: Techniques \u2014 University of Twente Research \nInformation Skip to main navigation Skip to search Skip to main content University of Twente \nResearch Information Logo Home Profiles Research Units Research Output Datasets \nActivities Prizes Press / Media Search by expertise, name or affiliation Report on the \ndevelopment issues: Techniques Maurice van Keulen, J. Vonk Databases (Former) \nResearch output: Book/Report \u203a Report \u203a Professional Overview Original language \nUndefined Place of Publication Enschede Publisher Databases (DB) Number of pages 70 \nPublication status Published - 2002 Publication series Name Deliverable D6 of the \nSUMMER project Keywords METIS-209753 Cite this APA Author BIBTEX Harvard Standard \nRIS Vancouver van Keulen, M., & Vonk, J. (2002). Report on the development issues: \nTechniques. (Deliverable D6 of the SUMMER project). (DB). , ; \u2026"}}, {"bib": {"title": "An integrated DMMDBMS prototype supporting video retrieval and security", "pub_year": 2002, "author": "Maurice van Keulen and J Vonk", "journal": "Deliverable D5 of the SUMMER project", "publisher": "Databases (DB)", "abstract": "An integrated DMMDBMS prototype supporting video retrieval and security \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research \nOutput Datasets Activities Prizes Press / Media Search by expertise, name or affiliation An \nintegrated DMMDBMS prototype supporting video retrieval and security Maurice van Keulen, J. \nVonk Databases (Former) Research output: Book/Report \u203a Report \u203a Professional Overview \nOriginal language English Place of Publication Enschede Publisher Centre for Telematics and \nInformation Technology (CTIT) Number of pages 30 Publication status Published - 2002 Cite \nthis APA Author BIBTEX Harvard Standard RIS Vancouver van Keulen, M., & Vonk, J. (2002). An \nintegrated DMMDBMS prototype supporting video retrieval and security. Centre for and (CTIT/\u2026"}}, {"bib": {"title": "SUMMER demo", "pub_year": 2001, "author": "Maurice van Keulen", "publisher": "KPN Research", "abstract": "SUMMER demo \u2014 University of Twente Research Information Skip to main navigation Skip to \nsearch Skip to main content University of Twente Research Information Logo Home Profiles \nResearch Units Research Output Datasets Activities Prizes Press / Media Search by expertise, \nname or affiliation SUMMER demo Maurice van Keulen Databases (Former) Research output: \nBook/Report \u203a Report \u203a Professional Overview Original language Undefined Place of \nPublication Enschede Publisher KPN Research Number of pages 48 Publication status \nPublished - 2001 Keywords METIS-203524 Cite this APA Author BIBTEX Harvard Standard \nRIS Vancouver van Keulen, M. (2001). SUMMER demo. KPN Research. van Keulen, Maurice. / \nSUMMER demo. Enschede : KPN Research, 2001. 48 p. van Keulen, M 2001, SUMMER demo. \nKPN Research, Enschede. SUMMER demo. / van Keulen, Maurice. Enschede : KPN , 2001. p/\u2026"}}, {"bib": {"title": "DMMDBMS components and architectures", "pub_year": 2001, "author": "Maurice van Keulen", "publisher": "KPN Research", "abstract": "DMMDBMS components and architectures (2001) | www.narcis.nl KNAW KNAW Narcis Back to \nsearch results University of Twente Publication DMMDBMS components and architectures \n(2001) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as \nEndNote Export to RefWorks Title DMMDBMS components and architectures Author van Keulen, \nMaurice Publisher Databases (Former) Date issued 2001 Access Restricted Access \nReference(s) METIS-203523 Language und Type Report Publisher KPN Research Publication \nhttps://research.utwente.nl/en/publications/dmmdbms-componen... Persistent Identifier \nurn:nbn:nl:ui:28-bfafc14a-5fcb-4882-891c-abc11c2d314a Metadata XML Source University of \nTwente Go to Website Navigation: Home about narcis login Nederlands contact Anna van \nSaksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl More >>> Youtube Newsletter >>> >>>\u2026"}}, {"bib": {"title": "Requirements analysis based on application domains", "pub_year": 2001, "author": "Maurice van Keulen", "publisher": "V2 (partner SUMMER project)", "abstract": "Requirements analysis based on application domains (2001) | www.narcis.nl KNAW KNAW \nNarcis Back to search results University of Twente Publication Requirements analysis based \non application domains (2001) Pagina-navigatie: Main Save publication Save as MODS \nExport to Mendeley Save as EndNote Export to RefWorks Title Requirements analysis based \non application domains Author van Keulen, Maurice Publisher Databases (Former) Date \nissued 2001 Access Restricted Access Language English Type Report Publisher V2 (partner \nSUMMER project) Publication https://research.utwente.nl/en/publications/requirements-ana... \nPersistent Identifier urn:nbn:nl:ui:28-2d57c94c-4e7f-481c-8981-1d48494516c7 Metadata XML \nSource University of Twente Go to Website Navigation: Home about narcis login Nederlands \ncontact Anna van Saksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl More >>> >>> >>\u2026"}}, {"bib": {"title": "XML: De eend met de gouden eieren?", "pub_year": 2001, "author": "Maurice Keulen", "journal": "I/O Vivat", "volume": "17", "number": "1", "pages": "27-34"}}, {"bib": {"title": "View integration", "pub_year": 1999, "author": "Herman Balsters and Maarten Fokkinga and Maurice Van Keulen", "abstract": "Problem statement Suppose that we are to build one information system (database, say) for several users that each have their own view of the world. The idea is to integrate the views of all individual users into one view (which is satisfactory for all users), and build the information system for that view. The problem that we are faced with is this:"}}, {"bib": {"title": "Object-Oriented Programming: A Unified Foundation-Guiseppe Castagna", "pub_year": 1998, "author": "Maurice van Keulen", "journal": "L'Objet", "volume": "4", "number": "1", "pages": "108-110", "abstract": "Object-Oriented Programming: A Unified Foundation - Guiseppe Castagna (1998) | www.narcis.nl \nKNAW KNAW Narcis Back to search results University of Twente Publication Object-Oriented \nProgramming: A Unified Foundation - Guiseppe Castagna (1998) Pagina-navigatie: Main \nSave publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks \nTitle Object-Oriented Programming: A Unified Foundation - Guiseppe Castagna Published in \nL'Objet, 4(1), 108 - 110. Lavoisier. ISSN 1262-1137. Author van Keulen, Maurice Publisher \nDatabases (Former) Date issued 1998-3 Access Restricted Access Reference(s) EWI-6308, \nDB-OODB: OBJECT-ORIENTED DATABASES Language und Type Book Review Publisher \nLavoisier Publication https://research.utwente.nl/en/publications/objectoriented-p... OpenURL \nSearch this publication in (your) library Persistent Identifier urn:nbn:nl:ui:28-a97efb0d----of \u2026"}}, {"bib": {"title": "Trends in tools voor gegevensmodellering", "pub_year": 1998, "author": "Maurice van Keulen and HW Brand", "journal": "Informatie", "volume": "40", "pages": "8-12", "abstract": "Trends in tools voor gegevensmodellering (1998) | www.narcis.nl KNAW KNAW Narcis Back \nto search results University of Twente Publication Trends in tools voor gegevensmodellering \n(1998) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save \nas EndNote Export to RefWorks Title Trends in tools voor gegevensmodellering Published \nin Informatie, 40, 8 - 12. SDU. ISSN 0019-9907. Author van Keulen, Maurice; Brand, HW \nPublisher Databases (Former) Date issued 1998-3 Access Restricted Access Reference(s) \nEWI-6309, DB-MOD: DATA MODELLING Language und Type Article Publisher SDU \nPublication https://research.utwente.nl/en/publications/trends-in-tools-... OpenURL Search \nthis publication in (your) library Persistent Identifier urn:nbn:nl:ui:28-67059fc2-b77e-4831-a33e-f4a999317c1c \nMetadata XML Source University of Twente Go to Website Navigation: Home about narcis @.\u2026"}}, {"bib": {"title": "Formal operation definition in object-oriented databases", "pub_year": 1997, "author": "Maurice van Keulen", "publisher": "Centre for Telematics and Information Technology University of Twente", "abstract": "Shall Earth no more inspire thee, Thou lonely dreamer now? Since passion may not fire thee Shall Nature cease to bow? Thy mind is ever moving In regions dark to thee; Recall its useless roving\u2014Come back and dwell with me\u2014I know my mountain breezes Enchant and soothe thee still\u2014I know my sunshine pleases Despite thy wayward will\u2014When day with evening blending Sinks from the summer sky, I\u2019ve seen thy spirit bending In fond idolatry\u2014I\u2019ve watched thee every hour\u2014I know my mighty sway\u2014I know my magic power To drive thy griefs away\u2014Few hearts to mortals given On earth so wildly pine Yet none would ask a Heaven More like this Earth than thine Then let my winds caress thee\u2014Thy comrade let me be\u2014Since nought beside can bless thee Return and dwell with me\u2014"}}, {"bib": {"title": "How do we type an object-oriented query result", "pub_year": 1996, "author": "H Balsters and Maurice van Keulen", "journal": "Dagstuhl Seminar Reports", "number": "10", "pages": "20-22", "abstract": "How do we type an object-oriented query result (abstract) (1996) | www.narcis.nl KNAW \nKNAW Narcis Back to search results University of Twente Publication How do we type an \nobject-oriented query result (abstract) (1996) Pagina-navigatie: Main Save publication Save \nas MODS Export to Mendeley Save as EndNote Export to RefWorks Title How do we type \nan object-oriented query result (abstract) Published in Dagstuhl Seminar Reports, 20 - 22. \nDagstuhl. ISSN 0940-1121. Author Balsters, H.; van Keulen, Maurice Date issued 1996 \nAccess Restricted Access Reference(s) METIS-122010 Language und Type Article \nPublisher Dagstuhl Publication https://research.utwente.nl/en/publications/how-do-we-type-a... \nOpenURL Search this publication in (your) library Persistent Identifier urn:nbn:nl:ui:28-44337cb5-8320-4cdf-a0a0-46b227897bef \nMetadata XML Source University of Twente Go to Website Navigation: Home @.\u2026"}}, {"bib": {"title": "An architecture and methodology for the design and development of Technical Information Systems", "pub_year": 1995, "author": "H Balsters and R Capobianchi and M Mautref and Maurice van Keulen", "journal": "CTIT Technical Report Series", "number": "95-21", "publisher": "Centre for Telematics and Information Technology (CTIT)"}}, {"bib": {"title": "TM Manual: version 2.0 revision e", "pub_year": 1995, "author": "H Balsters and RA de By and Maurice van Keulen and J Skowronek", "journal": "IMPRESS deliverable", "number": "UT-TECH-T79-001-R4", "publisher": "University of Twente", "abstract": "This is the TM language manual, describing version 2.0 of the database specification language TM. This document is a working document, which means that not all (although many) of our ideas concerning the construction of the language have been thoroughly evaluated. Most importantly, we lack a fine set of application case studies, which would undoubtedly result in further enhancements and tuning of the language. This is obviously a cyclic problem, because how can we obtain good case studies if there is no manual? Thus, the main goal of this document is to get some people out in the field of database specification use TM, and report on their findings.We use TM as a high-level language for the design and specification of object-oriented database schemas in an efficient and effective manner. The TM language and its accompanying design tools enable users to perform complex semantical analyses of schemas, thus paving the way to a complete debugging of the conceptual design. As a design language, TM is equipped with powerful structuring primitives which enable a user to arrive at natural and intuitively correct designs. These structuring primitives are characterized by the following features"}}, {"bib": {"title": "Design and Validation of Reliable Complex Computer Systems", "pub_year": 1995, "author": "Maurice van Keulen and J Skowronek and Peter MG Apers and H Balsters and Henk Blanken and RA de By and Jan Flokstra", "journal": "Memoranda informatica", "volume": "95", "number": "38", "abstract": "Design and Validation of Reliable Complex Computer Systems (1995) | www.narcis.nl \nKNAW KNAW Narcis Back to search results University of Twente Publication Design and \nValidation of Reliable Complex Computer Systems (1995) Pagina-navigatie: Main Save \npublication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title \nDesign and Validation of Reliable Complex Computer Systems Published in Memoranda \ninformatica, 95(38). University of Twente. ISSN 0924-3755. Author van Keulen, Maurice; \nSkowronek, J.; Apers, Peter MG; Balsters, H.; Blanken, Henk; de By, RA; Flokstra, Jan \nPublisher Databases (Former) Date issued 1995 Access Restricted Access Reference(s) \nMETIS-118764 Language und Type Article Publisher University of Twente Publication https://research.utwente.nl/en/publications/design-and-valid... \nOpenURL Search this publication in (your) library Persistent urn:nbn::\u2026"}}, {"bib": {"title": "Impress/UT-soft-D41-001-R1, Database Design Tool, deliverable S41", "pub_year": 1994, "author": "Peter MG Apers and Henk Blanken and H Balsters and Jan Flokstra and RT Boon and J Skowronek and Maurice van Keulen", "abstract": "Impress/UT-soft-D41-001-R1, Database Design Tool, deliverable S41 \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research \nOutput Datasets Activities Prizes Press / Media Search by expertise, name or affiliation \nImpress/UT-soft-D41-001-R1, Database Design Tool, deliverable S41 Peter MG Apers, \nHenk Blanken, H. Balsters, Jan Flokstra, RT Boon, J. Skowronek, Maurice van Keulen \nDatabases (Former) Research output: Other contribution \u203a Other research output Overview \nOriginal language Undefined Place of Publication Evry, Frankrijk Publication status \nPublished - 23 Jan 1994 Keywords METIS-123501 Cite this APA Author BIBTEX Harvard \nStandard RIS Vancouver Apers, PMG, Blanken, H., Balsters, H., Flokstra, J., Boon, RT, \nSkowronek, J., & van Keulen, M. (1994, Jan 23). \u2026"}}, {"bib": {"title": "Impress/UT-Soft-D34-001-R1, TMQL Implementation, deliverable S34", "pub_year": 1994, "author": "Peter MG Apers and Henk Blanken and H Balsters and Jan Flokstra and RT Boon and J Skowronek and Maurice van Keulen", "abstract": "Impress/UT-Soft-D34-001-R1, TMQL Implementation, deliverable S34 \u2014 University of \nTwente Research Information Skip to main navigation Skip to search Skip to main content \nUniversity of Twente Research Information Logo Home Profiles Research Units Research \nOutput Datasets Activities Prizes Press / Media Search by expertise, name or affiliation \nImpress/UT-Soft-D34-001-R1, TMQL Implementation, deliverable S34 Peter MG Apers, \nHenk Blanken, H. Balsters, Jan Flokstra, RT Boon, J. Skowronek, Maurice van Keulen \nDatabases (Former) Research output: Other contribution \u203a Other research output Overview \nOriginal language Undefined Place of Publication Evry, Frankrijk Publication status \nPublished - 23 Jan 1994 Keywords METIS-123502 Cite this APA Author BIBTEX Harvard \nStandard RIS Vancouver Apers, PMG, Blanken, H., Balsters, H., Flokstra, J., Boon, RT, \nSkowronek, J., & van Keulen, M. (1994, Jan 23). \u2026"}}, {"bib": {"title": "Results of an examination of the GTI", "pub_year": 1993, "author": "Maurice van Keulen", "abstract": "Results of an examination of the GTI (1993) | www.narcis.nl KNAW KNAW Narcis Back to \nsearch results University of Twente Publication Results of an examination of the GTI (1993) \nPagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote \nExport to RefWorks Title Results of an examination of the GTI Author van Keulen, Maurice Date \nissued 1993-11-02 Access Restricted Access Reference(s) METIS-123420 Language und \nType Other Publication https://research.utwente.nl/en/publications/results-of-an-ex... Persistent \nIdentifier urn:nbn:nl:ui:28-bd88f6de-a76d-48de-b4df-c9eed12930cb Metadata XML Source \nUniversity of Twente Go to Website Navigation: Home about narcis login Nederlands contact \nAnna van Saksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl More >>> Youtube \nNewsletter >>> Privacy statement >>> Disclaimer >>> DANS is an institute of KNAW and NWO \u2026"}}, {"bib": {"title": "Some methodological guidelines for IMPRESS", "pub_year": 1993, "author": "F Ardorino and M Mautref and RA de By and Maurice van Keulen", "abstract": "Some methodological guidelines for IMPRESS \u2014 University of Twente Research \nInformation Skip to main navigation Skip to search Skip to main content University of Twente \nResearch Information Logo Home Profiles Research Units Research Output Datasets \nActivities Prizes Press / Media Search by expertise, name or affiliation Some \nmethodological guidelines for IMPRESS F. Ardorino, M. Mautref, RA de By, Maurice van \nKeulen Databases (Former) Research output: Other contribution \u203a Other research output \nOverview Original language Undefined Place of Publication Enschede Publication status \nPublished - 1 Nov 1993 Keywords METIS-123417 Cite this APA Author BIBTEX Harvard \nStandard RIS Vancouver Ardorino, F., Mautref, M., de By, RA, & van Keulen, M. (1993, Nov 1). \nSome methodological guidelines for IMPRESS. Ardorino, F. ; Mautref, M. ; de By, RA ; van \nKeulen, Maurice. / Some methodological for . . . , F\u2026"}}, {"bib": {"title": "Global design of the prototyping environment", "pub_year": 1993, "author": "Maurice van Keulen", "journal": "IMPRESS/UT-MEMO", "number": "W4-002-R1", "publisher": "Universiteit Twente", "abstract": "Global design of the prototyping environment \u2014 University of Twente Research Information Skip \nto main navigation Skip to search Skip to main content University of Twente Research Information \nLogo Home Profiles Research Units Projects Research output Datasets Activities Prizes Press \n/ Media Search by expertise, name or affiliation Global design of the prototyping environment \nMaurice van Keulen Research output: Book/Report \u203a Report \u203a Professional Overview Original \nlanguage Undefined Place of Publication Enschede Publisher Universiteit Twente Publication \nstatus Published - 13 May 1993 Publication series Name IMPRESS/UT-MEMO No. W4-002-R1 \nKeywords METIS-123419 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver \nvan Keulen, M. (1993). Global design of the prototyping environment. (IMPRESS/UT-MEMO; \nNo. W4-002-R1). Universiteit Twente. van Keulen, Maurice. / Global design . (\u2026"}}, {"bib": {"title": "The TM manual, version 2.0", "pub_year": 1993, "author": "Peter MG Apers and RA de By and Henk Blanken and H Balsters and J Skowronek and HJ Steenhagen and Maurice van Keulen and AN Wilschut and Jan Flokstra and RJ Blok", "abstract": "The TM manual, version 2.0 (1993) | www.narcis.nl KNAW KNAW Narcis Back to search \nresults University of Twente Publication The TM manual, version 2.0 (1993) Pagina-navigatie: \nMain Save publication Save as MODS Export to Mendeley Save as EndNote Export to \nRefWorks Title The TM manual, version 2.0 Author Apers, Peter MG; de By, RA; Blanken, \nHenk; Balsters, H.; Skowronek, J.; Steenhagen, HJ; van Keulen, Maurice; Wilschut, AN; \nFlokstra, Jan; Blok, RJ Publisher Databases (Former) Date issued 1993-02-16 Access \nRestricted Access Reference(s) METIS-123418 Language und Type Other Publication https://research.utwente.nl/en/publications/the-tm-manual-ve... \nPersistent Identifier urn:nbn:nl:ui:28-25cc55fc-3da8-4607-a5c7-7b7d4b77e6a2 Metadata \nXML Source University of Twente Go to Website Navigation: Home about narcis login \nNederlands contact Anna van Saksenlaan 51 2593 HW Den Haag ..\u2026"}}, {"bib": {"title": "Struggle for LIFE: A Rapid prototype for TM in LIFE", "pub_year": 1992, "author": "M van Keulen"}}, {"bib": {"title": "Moa and the Multi-model Architecture: A New Perspective on NF2", "author": "M Keulen and J Vonk and AP Vries and J Flokstra and HE Blok", "abstract": "Advanced non-traditional application domains such as geographic information systems and digital library systems demand advanced data management support. In an effort to cope with this demand, we present the concept of a novel multi-model DBMS architecture which provides evaluation of queries on complexly structured data without sacrificing efficiency. A vital role in this architecture is played by the Moa language featuring a nested relational data model based on XNF           2, in which we placed renewed interest. Furthermore, extensibility in Moa avoids optimization obstacles due to black-box treatment of ADTs. The combination of a mapping of queries on complexly structured data to an efficient physical algebra expression via a nested relational algebra, extensibility open to optimization, and the consequently better integration of domain-specific algorithms, makes that the Moa system can efficiently and effectively handle complex queries from non-traditional application domains."}}, {"bib": {"title": "Data-Driven Process Discovery and Analysis", "author": "Paolo Ceravolo and Maurice van Keulen and Kilian Stoffel"}}, {"bib": {"title": "Pay-as-you-go data integration for bioinformatics", "author": "Brend Wanders Paul van der Vet and Maurice van Keulen", "abstract": "Brend Wanders Paul van der Vet Maurice van Keulen Page 1 Brend Wanders Paul van der Vet \nMaurice van Keulen <b.wanders@utwente.nl> Pay-as-you-go data integration for bioinformatics \nACO1 ACO2 IDH3A IDH3B IDH3G IDH1 IDH2 SUCLA2 SUCLG1 SUCLG2 SUCLA2P1 \nLOC283398 SDHA SDHB SDHD SDHC FH MDH1 MDH2 MDH1B CS SLC35G3 citrate \ncis-aconitate D-threo-isocitrate 3-carboxy-1-hydroxypropyl-ThPP succinyl-CoA 2-oxosuccinate \nsuccinate fumarate (S)-malate oxaloacetate ACO1 ACO2 IREB2 ACO1 ACO2 OGDH OGDHL \nDLST SUCLA2 SUCLG1 SUCLG2 SUCLA2 SUCLG1 SUCLG2 IDH1 IDH2 IDH2 IDH1 IDH2 \nIDH1 IDH2 OGDH OGDHL SDHA SDHB SDHD SDHC SDHA SDHB SDHD SDHC H2O H2O \nNAD H+ NADH NAD NADP CO2 NADPH H+ NADH CO2 H+ ubiquinol ubiquinone FADH2 \nFAD+ CoA ATP ADP GDP IDP ITP acetyl-CoA H2O H+ CoA NAD NADH H+ H2O Pi CoA Pi --\u2026"}}, {"bib": {"title": "Item Removed", "author": "Marko Smiljanic and Henk Blanken and Maurice van Keulen and Willem Jonker"}}, {"bib": {"title": "Workshop FlexDBIST 2011", "author": "Gloria Bordogna and Giuseppe Psaila and Elena Baralis and Patrick Bosc and Barbara Catania and Richard Chbeir and Eliseo Clementini and Karin Coninx and Alfredo Cuzzocrea and Ander de Keijzer and Guy de Tr\u00e9 and Fernando Ferri and Sara Foresti and Pablo Garcia Bringas and Paolo Garza and Patrizia Grifoni and Dion Hoe-Lian Goh and Enrique Herrera Viedma and Maria J Martin-Bautista and Rosa Meo and Barbara Oliboni and Javier Parapar and Gerardo Pelosi and Ilia Petrov and Olivier Pivert and Chiara Renso and Crawford Revie and Maurice van Keulen and Slawomir Zadrozny", "abstract": "Workshop FlexDBIST 2011 Organizing Committee IEEE.org Help Cart Jobs Board Create \nAccount Toggle navigation IEEE Computer Society Digital Library Jobs Tech News Resource \nCenter Press Room Browse By Date Advertising About Us IEEE IEEE Computer Society IEEE \nComputer Society Digital Library My Subscriptions Magazines Journals Conference \nProceedings Institutional Subscriptions IEEE IEEE Computer Society More Jobs Tech News \nResource Center Press Room Browse By Date Advertising About Us Cart All Advanced Search \nConference Cover Image Download 1.Home 2.Proceedings 3.dexa 2011 Workshop FlexDBIST \n2011 Organizing Committee 2011, pp. xxiii-xxiii, DOI Bookmark: 10.1109/DEXA.Keywords \nAuthors Abstract Provides a listing of current committee members. Workshop FlexDBIST 2011 \n,Organizing Committee , ,Program Committee Chairs ,Gloria Bordogna, ,CNR Consiglio delle , , \u2026"}}, {"bib": {"title": "Pay-\u2010as-\u2010you-\u2010go data integration for bio-\u2010informatics (PAYDIBI)", "author": "M van Keulen and P van der Vet", "abstract": "Scientific research in bio-\u2010informatics is often data-\u2010driven and supported by biological databases. A biological database contains factual information collected from scientific experiments and computational analyses about areas including genomics, proteomics, metabolomics, microarray gene expression, and phylogenetics. Information contained in biological databases includes gene function, structure, localization (both cellular and chromosomal), clinical effects of mutations as well as similarities of biological sequences and structures. 2 Much effort is involved in keeping them up-\u2010to-\u2010date, extending them and in creating new ones (see for example[1],[2]).In a growing number of research projects, researchers like to ask combined questions, ie, questions that require the combination of information from more than one database.[3] estimates that in scientific workflows\u201cin total 30% of all the tasks are data transformation tasks\u201d. Combining information from several biological databases can be a painstaking process, because of many reasons such as"}}]